{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\LLM\\git\\LLM\\Project\\Interview-Questions-Creator\n"
     ]
    }
   ],
   "source": [
    "%cd C:/LLM/git/LLM/Project/Interview-Questions-Creator\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/stats.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \\nMaryam Hadavand -Siri and Clayton V.Deutsch  \\n \\nThe c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \\ndata sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \\npredict spatial distribution and dependency between variables, correlation matrix is  a key element  to \\ndescribe this dependency . The principal directions of data set variance are defined by pri ncipal \\ncomponents. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \\neigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \\n Introduction  \\nDependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set 𝑋,𝑌 defined as:  \\n𝜌𝑋𝑌=𝐶𝑜𝑟𝑟 (𝑋,𝑌)= 𝐶𝑜𝑣 (𝑋,𝑌)\\n𝜎𝑋𝜎𝑌=𝐸[(𝑋−𝜇𝑋)(𝑌−𝜇𝑌]\\n𝜎𝑋𝜎𝑌 (1) \\n𝐶𝑜𝑣 (𝑋,𝑌)=1\\n𝑁� (𝑋𝑖𝑛\\n𝑖=1−𝜇𝑋)(𝑌𝑖−𝜇𝑌) (2) \\n𝜎𝑋=�∑ (𝑋𝑖−𝜇𝑋)𝑛\\n𝑖=1\\n𝑛          𝜎𝑌=�∑ (𝑌𝑖−𝜇𝑌)𝑛\\n𝑖=1\\n𝑛 (3) \\nWhere 𝜌,𝐸 𝑎𝑛𝑑  𝐶𝑜𝑣 , are correlation, expected value and covariance operator respectively, 𝜇   is the \\nmean, 𝜎 is standard deviation  and 𝑛 is number of variables.  The correlation is +1 in the case of a perfect \\npositive linear relationship, −1 in th e case of a perfect negative linear relationship,  and some value \\nbetween −1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \\nit approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \\neither −1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \\nsummarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \\nbetween different data sets and are symmetric as 𝐶𝑜𝑟𝑟 (𝑋,𝑌)=𝐶𝑜𝑟𝑟 (𝑌,𝑋). Correlation matrices must be \\npositive semi definite. It means for all Non -zero column vector  𝑍, 𝑍\\n𝑇𝜌𝑍>0 (𝑍𝑇 is the transpose of 𝑍 and  \\n𝜌 is correlation matrix).  \\nThe subject of multivariate analysis deals with the statistical analysis of the data collected on \\nmore than one variable. These variables may be correlated with each other, and their statistical \\ndependence is often taken into account when analyzing such data. Correlation matrix is a key element to \\nexplain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \\nwhich is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \\ndata.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, 𝑋≠0 is a vector in 𝐶\\n𝑛, and λ  is a scalar in C. Then 𝑋 is an eigenvector of A with \\neigenvalue λ, if 𝐴𝑋=λX.  The eigenvalues of a matrix  A are precisely the solutions λ to the characteristic \\nequation ( I is identity matrix).  \\ndet(𝐴−λI)=0 \\ndet(𝐴−λI)=det��𝑎11 0\\n0  𝑎22⋯   0\\n0   0\\n⋮   ⋮\\n0  0⋱  0\\n  0   𝑎𝑛𝑛�−λ�10\\n01… 0\\n… 0\\n⋮⋮\\n00⋱0\\n01��  \\n \\n \\n \\n(4) ', metadata={'source': 'data/stats.pdf', 'page': 0}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 2                        =𝑑𝑒𝑡�𝑎11−λ 0\\n0𝑎22−λ⋯           0\\n…           0\\n⋮            ⋮\\n0             0  ⋱           0\\n0   𝑎𝑛𝑛−λ� \\n                      =(𝑎11−λ)(𝑎22−λ)…(𝑎𝑛𝑛−λ)=0 \\n \\nThe solutions to this equation are the eigenvalues λ𝑖=𝑎𝑖𝑖 (i=1,2,…n).   When we deal with large size of \\nmatrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \\ntwo classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \\nPartial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \\napproximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \\n(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \\non PCA method. MDS and factorization will be covered in future works.  \\n \\nPrincipal Co mponent Analysis (PCA) : \\nPrincipal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \\ncovariance structure of a set of variables. In particular it allows us to identify the principal directions in \\nwhich the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \\nset by a much smaller number of “new” variables, named as principal components  (Gillies,D., ,2005) .  For \\ne x a m p l e ,  i n  ﬁ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \\nmeasured in the X -Y coordinate system.  \\nRed Circles:            𝑥1,𝑥2,… 𝑥𝑛                                        𝑀𝑒𝑎𝑛=𝜇𝑥 \\nBlue Circle            𝑦1,𝑦2,…,𝑦𝑛                                       𝑀𝑒𝑎𝑛=𝜇𝑦 \\nThe principal directi on in which the data varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (𝜇𝑥, 𝜇𝑦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \\na given set of data, principal component analysis ﬁnds the axis system deﬁned by the principal directions o\\nf  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ﬁ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \\ncomponents.  \\n \\n  \\n \\nFigure 1  Figure 2  \\nIf the variation in a data set is caused by some natural property, or is caused by random experimental \\nerror, then we may expect it to be normally distributed. In this case we show the nominal extent of the \\nnormal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \\ndata points that are thought of as belonging to a class. It is drawn at a distance beyond which the \\nprobability of a point belonging to the class is low, and can be thought of as a class boundary.  \\nIf the variation in the data is caused by some other relationship, then PCA gives us a way of \\nreducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \\nin ﬁgure2. As in ﬁgure1 the principal direction in which the data varies is shown by the U axis and the \\nsecondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \\nmay assume, for example, that they are only non -zero because of experimental noise  or measurement \\n  ', metadata={'source': 'data/stats.pdf', 'page': 1}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \\nhave reduced the  dimensionality of the problem by 1.  \\nIn computational terms the principal components are found by calculating the eigenvectors and \\neigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \\nthe co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \\nvariation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \\nvariation and so on. In the other word, eigenvalues are the variance of principal components. The first \\neigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \\nsecond principal component and so on (Gillies,D., 2005) . \\nThe eigenvalues of A, 𝑛 ×𝑛 matrix, are defined as the roots of:  \\ndet(𝐴−λ)=|𝐴−λI|=0 (5) \\nLet λ be an eigenvalue of A. Then there exists a vector 𝑥 such that:  \\n𝐴𝑥=λ𝑥 (6) \\nThe vector 𝑥  is called an eigenvector of A, associated with the eigenvalue λ . Notice that there is no \\nunique solution for 𝑥  in the above equation. It is a direction vector only and can be scaled to any \\nmagnitude. To find a numerical solution for 𝑥 we need to set one of its elements to an arbitrary value, say \\n1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \\nwe repeat the process with another element. Ordinarily we normalize the final values so that 𝑥  has length \\none, that is 𝑥 .𝑥𝑇=1. \\nSuppose we have a 3×3 matrix A with eigenvectors 𝑥1,𝑥2,𝑥3 and eigenvalues λ1,λ2,λ3 so: \\n𝐴𝑥1=λ1𝑥1                  𝐴𝑥2=𝜆2𝑥2                𝐴𝑥3=𝜆3𝑥3                 (7) \\nPutting the eigenvectors as the columns of a matrix gives:  \\n𝐴[𝑥1𝑥2𝑥3]=�λ1 0 0\\n0λ2 0\\n0 0λ3�[𝑥1𝑥2𝑥3]   (8) \\nWriting:  \\n𝜑=[𝑥1𝑥2𝑥3]                     ʌ=�λ1 0 0\\n0λ2 0\\n0 0λ3�  (9) \\nGives us the matrix equation:  \\n𝐴𝜑=ʌ𝜑  (10) \\nWe normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \\n𝜑𝜑𝑇=𝜑𝑇𝜑=𝐼  (5) \\nThis means that:  \\n𝜑𝑇𝐴𝜑=ʌ  (6) \\nAnd:  \\n𝐴=𝜑ʌ𝜑𝑇  (7) \\nN o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Σ  b e  a  𝑛×𝑛 \\ncovariance matrix. There is an orthogonal  𝑛 ×𝑛 matrix 𝜑 whose columns are eigenvectors of Σ and a \\ndiagonal matrix ʌ  whose diagonal elements are the eigenvalues of Σ, such that:  \\n𝜑Σ𝜑𝑇=ʌ  (8) \\nWe can look on the matrix of eigenvectors 𝜑 as a linear transformation which, in the example of figure1 \\ntransforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by 𝜑  transforms the data points into a data set where the variabl es are \\nuncorrelated. The correlation matrix of the data in the new coordinate system is ʌ which has zeros in all \\nthe off diagonal elements.  \\nPrincipal component analysis is appropriate when you have obtained measures on a number of \\nobserved variables and wi sh to develop a smaller number of artificial variables (called principal \\ncomponents) that will account for most of the variance in the observed variables.  Some limitations of PCA  \\n(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \\nonly consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \\nof PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \\nmatrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but ', metadata={'source': 'data/stats.pdf', 'page': 2}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \\nthe original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \\nvariance.  (5) PCA is not scale invariant.  \\n \\nImplementation  \\nIn this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables displayed in Figure 3 . This is an \\nappropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \\nFigure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \\n1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \\nwill find the correlation between the two correspond ing variables. For example, where the row for \\nvariable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \\ncorrelation between variables 1 and 2 is 0.14 . \\nBased on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \\n(𝜌25=0.76  ,𝜌26=0.66 ,𝜌56=0.56 ) . \\nVariable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (𝜌\\n42=−0.73  ,𝜌45=−0.59 ,𝜌46=−0.46 ). \\nVariable 3 also shows negative correlation with variab le 2, 5 and 6 \\n (𝜌32=−0.53  ,𝜌35=−0.38 , 𝜌36=−0.49 ). \\nHowever, variable 1 has no correlation with the other variables. When the correlation between two \\nvariables is less than 0.2 ( |𝜌|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \\nLet’s reorder variables based on their correlations to each other  and have a new correlation matrix for \\nreordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \\nsecond and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \\nplaces at the last order.  \\n1 Ni(V1)  Fe(V2)  \\n2 Fe(V2)  Co(V5)  \\n3 SiO2(V3)  Al2O3(V6)  \\n4 MgO(V4)  MgO(V4)  \\n5 Co(V5)  SiO2(V3)  \\n6 Al2O3(V6)  Ni(V1)  \\nTable1 : Reordering variables based on their correlation  \\n \\n \\nFigure 3 : Correlation matrix for six different variables       \\n Figure 4: Correlation matrix based on reorder ed variables  \\n', metadata={'source': 'data/stats.pdf', 'page': 3}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \\ngroups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \\ngroup, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \\nno correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \\nessence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \\nwith many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \\nsmaller set of artificial variables which called dimension reduction.  \\n \\nFigure 5: Three distinct groups of variables  \\n \\nEigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \\nare reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \\nFe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \\nSiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \\nMgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \\nCo(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \\nAl2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \\nTable 2: Eigenvalues and eigenvectors for correlation matrix  \\n Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative  eigenvalues  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nTable 3: Reordered eigenvalues and eigenvectors for correlation matrix  \\nCumulative eigenvalue curve c(x) is defined to be:  \\n𝐶(𝑥)=∑λ𝑖𝑥\\n𝑖\\n∑λ𝑖𝑁\\n𝑖=1×100  (15) \\nSecond group  \\nFirst  group  ', metadata={'source': 'data/stats.pdf', 'page': 4}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 6  \\nFigure 6: Cumulative eigenvalue VS Order of eigenvalues  \\nThe interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \\nmaintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \\nA feature transformation that, for instance, retains 50 percent of the original information (variance) of the \\ninput data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \\ninformation can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\n𝑖𝑓 �0.2≤𝜌≤1  𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒  𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 →𝐺𝑟𝑒𝑒𝑛\\n|𝜌|<0.2        𝑛𝑜 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛           →𝐺𝑟𝑎𝑦\\n−1≤𝜌≤−0.2𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒  𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 →𝑂𝑟𝑎𝑛𝑔𝑒 \\nThen, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \\neigenvectors ( λ=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \\nvariable 3 and 4 and de -correlation for variable 2  as expected . \\n  Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative eigenvalues/Number of variables  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nTable 4 : Reordering eigenvectors based on variable correlation (Table1)  \\nSummary and Future work  \\nThe c orrelation matrix summarizes correlation among several variables can describe statistical \\ndependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \\ncalculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \\ncalculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \\nmethods such as Multi -Dimensi onal scaling and factorization will be described.  \\n \\nReferences  \\nBabak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \\nCorrelated Data.  \\nGillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \\nIzenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 3 4 5 6Eigenvalues cummulative  \\nOrder of Eigenvalues  ', metadata={'source': 'data/stats.pdf', 'page': 5})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "\n",
    "for page in data:\n",
    "    question_gen = question_gen + page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \\nMaryam Hadavand -Siri and Clayton V.Deutsch  \\n \\nThe c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \\ndata sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \\npredict spatial distribution and dependency between variables, correlation matrix is  a key element  to \\ndescribe this dependency . The principal directions of data set variance are defined by pri ncipal \\ncomponents. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \\neigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \\n Introduction  \\nDependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set 𝑋,𝑌 defined as:  \\n𝜌𝑋𝑌=𝐶𝑜𝑟𝑟 (𝑋,𝑌)= 𝐶𝑜𝑣 (𝑋,𝑌)\\n𝜎𝑋𝜎𝑌=𝐸[(𝑋−𝜇𝑋)(𝑌−𝜇𝑌]\\n𝜎𝑋𝜎𝑌 (1) \\n𝐶𝑜𝑣 (𝑋,𝑌)=1\\n𝑁� (𝑋𝑖𝑛\\n𝑖=1−𝜇𝑋)(𝑌𝑖−𝜇𝑌) (2) \\n𝜎𝑋=�∑ (𝑋𝑖−𝜇𝑋)𝑛\\n𝑖=1\\n𝑛          𝜎𝑌=�∑ (𝑌𝑖−𝜇𝑌)𝑛\\n𝑖=1\\n𝑛 (3) \\nWhere 𝜌,𝐸 𝑎𝑛𝑑  𝐶𝑜𝑣 , are correlation, expected value and covariance operator respectively, 𝜇   is the \\nmean, 𝜎 is standard deviation  and 𝑛 is number of variables.  The correlation is +1 in the case of a perfect \\npositive linear relationship, −1 in th e case of a perfect negative linear relationship,  and some value \\nbetween −1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \\nit approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \\neither −1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \\nsummarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \\nbetween different data sets and are symmetric as 𝐶𝑜𝑟𝑟 (𝑋,𝑌)=𝐶𝑜𝑟𝑟 (𝑌,𝑋). Correlation matrices must be \\npositive semi definite. It means for all Non -zero column vector  𝑍, 𝑍\\n𝑇𝜌𝑍>0 (𝑍𝑇 is the transpose of 𝑍 and  \\n𝜌 is correlation matrix).  \\nThe subject of multivariate analysis deals with the statistical analysis of the data collected on \\nmore than one variable. These variables may be correlated with each other, and their statistical \\ndependence is often taken into account when analyzing such data. Correlation matrix is a key element to \\nexplain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \\nwhich is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \\ndata.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, 𝑋≠0 is a vector in 𝐶\\n𝑛, and λ  is a scalar in C. Then 𝑋 is an eigenvector of A with \\neigenvalue λ, if 𝐴𝑋=λX.  The eigenvalues of a matrix  A are precisely the solutions λ to the characteristic \\nequation ( I is identity matrix).  \\ndet(𝐴−λI)=0 \\ndet(𝐴−λI)=det��𝑎11 0\\n0  𝑎22⋯   0\\n0   0\\n⋮   ⋮\\n0  0⋱  0\\n  0   𝑎𝑛𝑛�−λ�10\\n01… 0\\n… 0\\n⋮⋮\\n00⋱0\\n01��  \\n \\n \\n \\n(4) Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 2                        =𝑑𝑒𝑡�𝑎11−λ 0\\n0𝑎22−λ⋯           0\\n…           0\\n⋮            ⋮\\n0             0  ⋱           0\\n0   𝑎𝑛𝑛−λ� \\n                      =(𝑎11−λ)(𝑎22−λ)…(𝑎𝑛𝑛−λ)=0 \\n \\nThe solutions to this equation are the eigenvalues λ𝑖=𝑎𝑖𝑖 (i=1,2,…n).   When we deal with large size of \\nmatrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \\ntwo classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \\nPartial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \\napproximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \\n(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \\non PCA method. MDS and factorization will be covered in future works.  \\n \\nPrincipal Co mponent Analysis (PCA) : \\nPrincipal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \\ncovariance structure of a set of variables. In particular it allows us to identify the principal directions in \\nwhich the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \\nset by a much smaller number of “new” variables, named as principal components  (Gillies,D., ,2005) .  For \\ne x a m p l e ,  i n  ﬁ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \\nmeasured in the X -Y coordinate system.  \\nRed Circles:            𝑥1,𝑥2,… 𝑥𝑛                                        𝑀𝑒𝑎𝑛=𝜇𝑥 \\nBlue Circle            𝑦1,𝑦2,…,𝑦𝑛                                       𝑀𝑒𝑎𝑛=𝜇𝑦 \\nThe principal directi on in which the data varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (𝜇𝑥, 𝜇𝑦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \\na given set of data, principal component analysis ﬁnds the axis system deﬁned by the principal directions o\\nf  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ﬁ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \\ncomponents.  \\n \\n  \\n \\nFigure 1  Figure 2  \\nIf the variation in a data set is caused by some natural property, or is caused by random experimental \\nerror, then we may expect it to be normally distributed. In this case we show the nominal extent of the \\nnormal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \\ndata points that are thought of as belonging to a class. It is drawn at a distance beyond which the \\nprobability of a point belonging to the class is low, and can be thought of as a class boundary.  \\nIf the variation in the data is caused by some other relationship, then PCA gives us a way of \\nreducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \\nin ﬁgure2. As in ﬁgure1 the principal direction in which the data varies is shown by the U axis and the \\nsecondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \\nmay assume, for example, that they are only non -zero because of experimental noise  or measurement \\n  Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \\nhave reduced the  dimensionality of the problem by 1.  \\nIn computational terms the principal components are found by calculating the eigenvectors and \\neigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \\nthe co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \\nvariation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \\nvariation and so on. In the other word, eigenvalues are the variance of principal components. The first \\neigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \\nsecond principal component and so on (Gillies,D., 2005) . \\nThe eigenvalues of A, 𝑛 ×𝑛 matrix, are defined as the roots of:  \\ndet(𝐴−λ)=|𝐴−λI|=0 (5) \\nLet λ be an eigenvalue of A. Then there exists a vector 𝑥 such that:  \\n𝐴𝑥=λ𝑥 (6) \\nThe vector 𝑥  is called an eigenvector of A, associated with the eigenvalue λ . Notice that there is no \\nunique solution for 𝑥  in the above equation. It is a direction vector only and can be scaled to any \\nmagnitude. To find a numerical solution for 𝑥 we need to set one of its elements to an arbitrary value, say \\n1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \\nwe repeat the process with another element. Ordinarily we normalize the final values so that 𝑥  has length \\none, that is 𝑥 .𝑥𝑇=1. \\nSuppose we have a 3×3 matrix A with eigenvectors 𝑥1,𝑥2,𝑥3 and eigenvalues λ1,λ2,λ3 so: \\n𝐴𝑥1=λ1𝑥1                  𝐴𝑥2=𝜆2𝑥2                𝐴𝑥3=𝜆3𝑥3                 (7) \\nPutting the eigenvectors as the columns of a matrix gives:  \\n𝐴[𝑥1𝑥2𝑥3]=�λ1 0 0\\n0λ2 0\\n0 0λ3�[𝑥1𝑥2𝑥3]   (8) \\nWriting:  \\n𝜑=[𝑥1𝑥2𝑥3]                     ʌ=�λ1 0 0\\n0λ2 0\\n0 0λ3�  (9) \\nGives us the matrix equation:  \\n𝐴𝜑=ʌ𝜑  (10) \\nWe normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \\n𝜑𝜑𝑇=𝜑𝑇𝜑=𝐼  (5) \\nThis means that:  \\n𝜑𝑇𝐴𝜑=ʌ  (6) \\nAnd:  \\n𝐴=𝜑ʌ𝜑𝑇  (7) \\nN o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Σ  b e  a  𝑛×𝑛 \\ncovariance matrix. There is an orthogonal  𝑛 ×𝑛 matrix 𝜑 whose columns are eigenvectors of Σ and a \\ndiagonal matrix ʌ  whose diagonal elements are the eigenvalues of Σ, such that:  \\n𝜑Σ𝜑𝑇=ʌ  (8) \\nWe can look on the matrix of eigenvectors 𝜑 as a linear transformation which, in the example of figure1 \\ntransforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by 𝜑  transforms the data points into a data set where the variabl es are \\nuncorrelated. The correlation matrix of the data in the new coordinate system is ʌ which has zeros in all \\nthe off diagonal elements.  \\nPrincipal component analysis is appropriate when you have obtained measures on a number of \\nobserved variables and wi sh to develop a smaller number of artificial variables (called principal \\ncomponents) that will account for most of the variance in the observed variables.  Some limitations of PCA  \\n(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \\nonly consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \\nof PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \\nmatrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \\nthe original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \\nvariance.  (5) PCA is not scale invariant.  \\n \\nImplementation  \\nIn this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables displayed in Figure 3 . This is an \\nappropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \\nFigure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \\n1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \\nwill find the correlation between the two correspond ing variables. For example, where the row for \\nvariable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \\ncorrelation between variables 1 and 2 is 0.14 . \\nBased on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \\n(𝜌25=0.76  ,𝜌26=0.66 ,𝜌56=0.56 ) . \\nVariable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (𝜌\\n42=−0.73  ,𝜌45=−0.59 ,𝜌46=−0.46 ). \\nVariable 3 also shows negative correlation with variab le 2, 5 and 6 \\n (𝜌32=−0.53  ,𝜌35=−0.38 , 𝜌36=−0.49 ). \\nHowever, variable 1 has no correlation with the other variables. When the correlation between two \\nvariables is less than 0.2 ( |𝜌|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \\nLet’s reorder variables based on their correlations to each other  and have a new correlation matrix for \\nreordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \\nsecond and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \\nplaces at the last order.  \\n1 Ni(V1)  Fe(V2)  \\n2 Fe(V2)  Co(V5)  \\n3 SiO2(V3)  Al2O3(V6)  \\n4 MgO(V4)  MgO(V4)  \\n5 Co(V5)  SiO2(V3)  \\n6 Al2O3(V6)  Ni(V1)  \\nTable1 : Reordering variables based on their correlation  \\n \\n \\nFigure 3 : Correlation matrix for six different variables       \\n Figure 4: Correlation matrix based on reorder ed variables  \\nPaper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \\ngroups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \\ngroup, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \\nno correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \\nessence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \\nwith many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \\nsmaller set of artificial variables which called dimension reduction.  \\n \\nFigure 5: Three distinct groups of variables  \\n \\nEigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \\nare reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \\nFe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \\nSiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \\nMgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \\nCo(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \\nAl2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \\nTable 2: Eigenvalues and eigenvectors for correlation matrix  \\n Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative  eigenvalues  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nTable 3: Reordered eigenvalues and eigenvectors for correlation matrix  \\nCumulative eigenvalue curve c(x) is defined to be:  \\n𝐶(𝑥)=∑λ𝑖𝑥\\n𝑖\\n∑λ𝑖𝑁\\n𝑖=1×100  (15) \\nSecond group  \\nFirst  group  Paper 408, CCG Annual Report 14 , 2012 (© 2012)  \\n408- 6  \\nFigure 6: Cumulative eigenvalue VS Order of eigenvalues  \\nThe interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \\nmaintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \\nA feature transformation that, for instance, retains 50 percent of the original information (variance) of the \\ninput data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \\ninformation can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\n𝑖𝑓 �0.2≤𝜌≤1  𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒  𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 →𝐺𝑟𝑒𝑒𝑛\\n|𝜌|<0.2        𝑛𝑜 𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛           →𝐺𝑟𝑎𝑦\\n−1≤𝜌≤−0.2𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒  𝑐𝑜𝑟𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 →𝑂𝑟𝑎𝑛𝑔𝑒 \\nThen, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \\neigenvectors ( λ=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \\nvariable 3 and 4 and de -correlation for variable 2  as expected . \\n  Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative eigenvalues/Number of variables  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nTable 4 : Reordering eigenvectors based on variable correlation (Table1)  \\nSummary and Future work  \\nThe c orrelation matrix summarizes correlation among several variables can describe statistical \\ndependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \\ncalculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \\ncalculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \\nmethods such as Multi -Dimensi onal scaling and factorization will be described.  \\n \\nReferences  \\nBabak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \\nCorrelated Data.  \\nGillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \\nIzenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 3 4 5 6Eigenvalues cummulative  \\nOrder of Eigenvalues  '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TextSplitter(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = False,\n",
    "    separator = \"\\n\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
