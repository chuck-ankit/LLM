{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\LLM\\\\git\\\\LLM\\\\Project\\\\Interview-Questions-Creator\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\LLM\\git\\LLM\\Project\\Interview-Questions-Creator\n"
     ]
    }
   ],
   "source": [
    "%cd C:/LLM/git/LLM/Project/Interview-Questions-Creator\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/stats.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \\nMaryam Hadavand -Siri and Clayton V.Deutsch  \\n \\nThe c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \\ndata sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \\npredict spatial distribution and dependency between variables, correlation matrix is  a key element  to \\ndescribe this dependency . The principal directions of data set variance are defined by pri ncipal \\ncomponents. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \\neigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \\n Introduction  \\nDependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set ğ‘‹,ğ‘Œ defined as:  \\nğœŒğ‘‹ğ‘Œ=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)= ğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)\\nğœğ‘‹ğœğ‘Œ=ğ¸[(ğ‘‹âˆ’ğœ‡ğ‘‹)(ğ‘Œâˆ’ğœ‡ğ‘Œ]\\nğœğ‘‹ğœğ‘Œ (1) \\nğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)=1\\nğ‘ï¿½ (ğ‘‹ğ‘–ğ‘›\\nğ‘–=1âˆ’ğœ‡ğ‘‹)(ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ) (2) \\nğœğ‘‹=ï¿½âˆ‘ (ğ‘‹ğ‘–âˆ’ğœ‡ğ‘‹)ğ‘›\\nğ‘–=1\\nğ‘›          ğœğ‘Œ=ï¿½âˆ‘ (ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ)ğ‘›\\nğ‘–=1\\nğ‘› (3) \\nWhere ğœŒ,ğ¸ ğ‘ğ‘›ğ‘‘  ğ¶ğ‘œğ‘£ , are correlation, expected value and covariance operator respectively, ğœ‡   is the \\nmean, ğœ is standard deviation  and ğ‘› is number of variables.  The correlation is +1 in the case of a perfect \\npositive linear relationship, âˆ’1 in th e case of a perfect negative linear relationship,  and some value \\nbetween âˆ’1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \\nit approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \\neither âˆ’1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \\nsummarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \\nbetween different data sets and are symmetric as ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘Œ,ğ‘‹). Correlation matrices must be \\npositive semi definite. It means for all Non -zero column vector  ğ‘, ğ‘\\nğ‘‡ğœŒğ‘>0 (ğ‘ğ‘‡ is the transpose of ğ‘ and  \\nğœŒ is correlation matrix).  \\nThe subject of multivariate analysis deals with the statistical analysis of the data collected on \\nmore than one variable. These variables may be correlated with each other, and their statistical \\ndependence is often taken into account when analyzing such data. Correlation matrix is a key element to \\nexplain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \\nwhich is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \\ndata.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, ğ‘‹â‰ 0 is a vector in ğ¶\\nğ‘›, and Î»  is a scalar in C. Then ğ‘‹ is an eigenvector of A with \\neigenvalue Î», if ğ´ğ‘‹=Î»X.  The eigenvalues of a matrix  A are precisely the solutions Î» to the characteristic \\nequation ( I is identity matrix).  \\ndet(ğ´âˆ’Î»I)=0 \\ndet(ğ´âˆ’Î»I)=detï¿½ï¿½ğ‘11 0\\n0  ğ‘22â‹¯   0\\n0   0\\nâ‹®   â‹®\\n0  0â‹±  0\\n  0   ğ‘ğ‘›ğ‘›ï¿½âˆ’Î»ï¿½10\\n01â€¦ 0\\nâ€¦ 0\\nâ‹®â‹®\\n00â‹±0\\n01ï¿½ï¿½  \\n \\n \\n \\n(4) ', metadata={'source': 'data/stats.pdf', 'page': 0}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 2                        =ğ‘‘ğ‘’ğ‘¡ï¿½ğ‘11âˆ’Î» 0\\n0ğ‘22âˆ’Î»â‹¯           0\\nâ€¦           0\\nâ‹®            â‹®\\n0             0  â‹±           0\\n0   ğ‘ğ‘›ğ‘›âˆ’Î»ï¿½ \\n                      =(ğ‘11âˆ’Î»)(ğ‘22âˆ’Î»)â€¦(ğ‘ğ‘›ğ‘›âˆ’Î»)=0 \\n \\nThe solutions to this equation are the eigenvalues Î»ğ‘–=ğ‘ğ‘–ğ‘– (i=1,2,â€¦n).   When we deal with large size of \\nmatrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \\ntwo classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \\nPartial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \\napproximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \\n(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \\non PCA method. MDS and factorization will be covered in future works.  \\n \\nPrincipal Co mponent Analysis (PCA) : \\nPrincipal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \\ncovariance structure of a set of variables. In particular it allows us to identify the principal directions in \\nwhich the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \\nset by a much smaller number of â€œnewâ€ variables, named as principal components  (Gillies,D., ,2005) .  For \\ne x a m p l e ,  i n  ï¬ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \\nmeasured in the X -Y coordinate system.  \\nRed Circles:            ğ‘¥1,ğ‘¥2,â€¦ ğ‘¥ğ‘›                                        ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¥ \\nBlue Circle            ğ‘¦1,ğ‘¦2,â€¦,ğ‘¦ğ‘›                                       ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¦ \\nThe principal directi on in which the data varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \\na given set of data, principal component analysis ï¬nds the axis system deï¬ned by the principal directions o\\nf  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ï¬ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \\ncomponents.  \\n \\n  \\n \\nFigure 1  Figure 2  \\nIf the variation in a data set is caused by some natural property, or is caused by random experimental \\nerror, then we may expect it to be normally distributed. In this case we show the nominal extent of the \\nnormal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \\ndata points that are thought of as belonging to a class. It is drawn at a distance beyond which the \\nprobability of a point belonging to the class is low, and can be thought of as a class boundary.  \\nIf the variation in the data is caused by some other relationship, then PCA gives us a way of \\nreducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \\nin ï¬gure2. As in ï¬gure1 the principal direction in which the data varies is shown by the U axis and the \\nsecondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \\nmay assume, for example, that they are only non -zero because of experimental noise  or measurement \\n  ', metadata={'source': 'data/stats.pdf', 'page': 1}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \\nhave reduced the  dimensionality of the problem by 1.  \\nIn computational terms the principal components are found by calculating the eigenvectors and \\neigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \\nthe co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \\nvariation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \\nvariation and so on. In the other word, eigenvalues are the variance of principal components. The first \\neigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \\nsecond principal component and so on (Gillies,D., 2005) . \\nThe eigenvalues of A, ğ‘› Ã—ğ‘› matrix, are defined as the roots of:  \\ndet(ğ´âˆ’Î»)=|ğ´âˆ’Î»I|=0 (5) \\nLet Î» be an eigenvalue of A. Then there exists a vector ğ‘¥ such that:  \\nğ´ğ‘¥=Î»ğ‘¥ (6) \\nThe vector ğ‘¥  is called an eigenvector of A, associated with the eigenvalue Î» . Notice that there is no \\nunique solution for ğ‘¥  in the above equation. It is a direction vector only and can be scaled to any \\nmagnitude. To find a numerical solution for ğ‘¥ we need to set one of its elements to an arbitrary value, say \\n1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \\nwe repeat the process with another element. Ordinarily we normalize the final values so that ğ‘¥  has length \\none, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \\nSuppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \\nğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ğ‘¥3=ğœ†3ğ‘¥3                 (7) \\nPutting the eigenvectors as the columns of a matrix gives:  \\nğ´[ğ‘¥1ğ‘¥2ğ‘¥3]=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½[ğ‘¥1ğ‘¥2ğ‘¥3]   (8) \\nWriting:  \\nğœ‘=[ğ‘¥1ğ‘¥2ğ‘¥3]                     ÊŒ=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½  (9) \\nGives us the matrix equation:  \\nğ´ğœ‘=ÊŒğœ‘  (10) \\nWe normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \\nğœ‘ğœ‘ğ‘‡=ğœ‘ğ‘‡ğœ‘=ğ¼  (5) \\nThis means that:  \\nğœ‘ğ‘‡ğ´ğœ‘=ÊŒ  (6) \\nAnd:  \\nğ´=ğœ‘ÊŒğœ‘ğ‘‡  (7) \\nN o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Î£  b e  a  ğ‘›Ã—ğ‘› \\ncovariance matrix. There is an orthogonal  ğ‘› Ã—ğ‘› matrix ğœ‘ whose columns are eigenvectors of Î£ and a \\ndiagonal matrix ÊŒ  whose diagonal elements are the eigenvalues of Î£, such that:  \\nğœ‘Î£ğœ‘ğ‘‡=ÊŒ  (8) \\nWe can look on the matrix of eigenvectors ğœ‘ as a linear transformation which, in the example of figure1 \\ntransforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by ğœ‘  transforms the data points into a data set where the variabl es are \\nuncorrelated. The correlation matrix of the data in the new coordinate system is ÊŒ which has zeros in all \\nthe off diagonal elements.  \\nPrincipal component analysis is appropriate when you have obtained measures on a number of \\nobserved variables and wi sh to develop a smaller number of artificial variables (called principal \\ncomponents) that will account for most of the variance in the observed variables.  Some limitations of PCA  \\n(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \\nonly consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \\nof PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \\nmatrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but ', metadata={'source': 'data/stats.pdf', 'page': 2}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \\nthe original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \\nvariance.  (5) PCA is not scale invariant.  \\n \\nImplementation  \\nIn this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables displayed in Figure 3 . This is an \\nappropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \\nFigure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \\n1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \\nwill find the correlation between the two correspond ing variables. For example, where the row for \\nvariable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \\ncorrelation between variables 1 and 2 is 0.14 . \\nBased on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \\n(ğœŒ25=0.76  ,ğœŒ26=0.66 ,ğœŒ56=0.56 ) . \\nVariable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (ğœŒ\\n42=âˆ’0.73  ,ğœŒ45=âˆ’0.59 ,ğœŒ46=âˆ’0.46 ). \\nVariable 3 also shows negative correlation with variab le 2, 5 and 6 \\n (ğœŒ32=âˆ’0.53  ,ğœŒ35=âˆ’0.38 , ğœŒ36=âˆ’0.49 ). \\nHowever, variable 1 has no correlation with the other variables. When the correlation between two \\nvariables is less than 0.2 ( |ğœŒ|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \\nLetâ€™s reorder variables based on their correlations to each other  and have a new correlation matrix for \\nreordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \\nsecond and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \\nplaces at the last order.  \\n1 Ni(V1)  Fe(V2)  \\n2 Fe(V2)  Co(V5)  \\n3 SiO2(V3)  Al2O3(V6)  \\n4 MgO(V4)  MgO(V4)  \\n5 Co(V5)  SiO2(V3)  \\n6 Al2O3(V6)  Ni(V1)  \\nTable1 : Reordering variables based on their correlation  \\n \\n \\nFigure 3 : Correlation matrix for six different variables       \\n Figure 4: Correlation matrix based on reorder ed variables  \\n', metadata={'source': 'data/stats.pdf', 'page': 3}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \\ngroups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \\ngroup, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \\nno correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \\nessence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \\nwith many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \\nsmaller set of artificial variables which called dimension reduction.  \\n \\nFigure 5: Three distinct groups of variables  \\n \\nEigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \\nare reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \\nFe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \\nSiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \\nMgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \\nCo(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \\nAl2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \\nTable 2: Eigenvalues and eigenvectors for correlation matrix  \\n Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative  eigenvalues  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nTable 3: Reordered eigenvalues and eigenvectors for correlation matrix  \\nCumulative eigenvalue curve c(x) is defined to be:  \\nğ¶(ğ‘¥)=âˆ‘Î»ğ‘–ğ‘¥\\nğ‘–\\nâˆ‘Î»ğ‘–ğ‘\\nğ‘–=1Ã—100  (15) \\nSecond group  \\nFirst  group  ', metadata={'source': 'data/stats.pdf', 'page': 4}),\n",
       " Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 6  \\nFigure 6: Cumulative eigenvalue VS Order of eigenvalues  \\nThe interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \\nmaintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \\nA feature transformation that, for instance, retains 50 percent of the original information (variance) of the \\ninput data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \\ninformation can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\nğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğºğ‘Ÿğ‘’ğ‘’ğ‘›\\n|ğœŒ|<0.2        ğ‘›ğ‘œ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›           â†’ğºğ‘Ÿğ‘ğ‘¦\\nâˆ’1â‰¤ğœŒâ‰¤âˆ’0.2ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğ‘‚ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ \\nThen, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \\neigenvectors ( Î»=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \\nvariable 3 and 4 and de -correlation for variable 2  as expected . \\n  Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative eigenvalues/Number of variables  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nTable 4 : Reordering eigenvectors based on variable correlation (Table1)  \\nSummary and Future work  \\nThe c orrelation matrix summarizes correlation among several variables can describe statistical \\ndependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \\ncalculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \\ncalculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \\nmethods such as Multi -Dimensi onal scaling and factorization will be described.  \\n \\nReferences  \\nBabak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \\nCorrelated Data.  \\nGillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \\nIzenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 3 4 5 6Eigenvalues cummulative  \\nOrder of Eigenvalues  ', metadata={'source': 'data/stats.pdf', 'page': 5})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "\n",
    "for page in data:\n",
    "    question_gen = question_gen + page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \\nMaryam Hadavand -Siri and Clayton V.Deutsch  \\n \\nThe c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \\ndata sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \\npredict spatial distribution and dependency between variables, correlation matrix is  a key element  to \\ndescribe this dependency . The principal directions of data set variance are defined by pri ncipal \\ncomponents. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \\neigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \\n Introduction  \\nDependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set ğ‘‹,ğ‘Œ defined as:  \\nğœŒğ‘‹ğ‘Œ=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)= ğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)\\nğœğ‘‹ğœğ‘Œ=ğ¸[(ğ‘‹âˆ’ğœ‡ğ‘‹)(ğ‘Œâˆ’ğœ‡ğ‘Œ]\\nğœğ‘‹ğœğ‘Œ (1) \\nğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)=1\\nğ‘ï¿½ (ğ‘‹ğ‘–ğ‘›\\nğ‘–=1âˆ’ğœ‡ğ‘‹)(ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ) (2) \\nğœğ‘‹=ï¿½âˆ‘ (ğ‘‹ğ‘–âˆ’ğœ‡ğ‘‹)ğ‘›\\nğ‘–=1\\nğ‘›          ğœğ‘Œ=ï¿½âˆ‘ (ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ)ğ‘›\\nğ‘–=1\\nğ‘› (3) \\nWhere ğœŒ,ğ¸ ğ‘ğ‘›ğ‘‘  ğ¶ğ‘œğ‘£ , are correlation, expected value and covariance operator respectively, ğœ‡   is the \\nmean, ğœ is standard deviation  and ğ‘› is number of variables.  The correlation is +1 in the case of a perfect \\npositive linear relationship, âˆ’1 in th e case of a perfect negative linear relationship,  and some value \\nbetween âˆ’1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \\nit approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \\neither âˆ’1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \\nsummarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \\nbetween different data sets and are symmetric as ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘Œ,ğ‘‹). Correlation matrices must be \\npositive semi definite. It means for all Non -zero column vector  ğ‘, ğ‘\\nğ‘‡ğœŒğ‘>0 (ğ‘ğ‘‡ is the transpose of ğ‘ and  \\nğœŒ is correlation matrix).  \\nThe subject of multivariate analysis deals with the statistical analysis of the data collected on \\nmore than one variable. These variables may be correlated with each other, and their statistical \\ndependence is often taken into account when analyzing such data. Correlation matrix is a key element to \\nexplain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \\nwhich is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \\ndata.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, ğ‘‹â‰ 0 is a vector in ğ¶\\nğ‘›, and Î»  is a scalar in C. Then ğ‘‹ is an eigenvector of A with \\neigenvalue Î», if ğ´ğ‘‹=Î»X.  The eigenvalues of a matrix  A are precisely the solutions Î» to the characteristic \\nequation ( I is identity matrix).  \\ndet(ğ´âˆ’Î»I)=0 \\ndet(ğ´âˆ’Î»I)=detï¿½ï¿½ğ‘11 0\\n0  ğ‘22â‹¯   0\\n0   0\\nâ‹®   â‹®\\n0  0â‹±  0\\n  0   ğ‘ğ‘›ğ‘›ï¿½âˆ’Î»ï¿½10\\n01â€¦ 0\\nâ€¦ 0\\nâ‹®â‹®\\n00â‹±0\\n01ï¿½ï¿½  \\n \\n \\n \\n(4) Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 2                        =ğ‘‘ğ‘’ğ‘¡ï¿½ğ‘11âˆ’Î» 0\\n0ğ‘22âˆ’Î»â‹¯           0\\nâ€¦           0\\nâ‹®            â‹®\\n0             0  â‹±           0\\n0   ğ‘ğ‘›ğ‘›âˆ’Î»ï¿½ \\n                      =(ğ‘11âˆ’Î»)(ğ‘22âˆ’Î»)â€¦(ğ‘ğ‘›ğ‘›âˆ’Î»)=0 \\n \\nThe solutions to this equation are the eigenvalues Î»ğ‘–=ğ‘ğ‘–ğ‘– (i=1,2,â€¦n).   When we deal with large size of \\nmatrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \\ntwo classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \\nPartial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \\napproximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \\n(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \\non PCA method. MDS and factorization will be covered in future works.  \\n \\nPrincipal Co mponent Analysis (PCA) : \\nPrincipal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \\ncovariance structure of a set of variables. In particular it allows us to identify the principal directions in \\nwhich the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \\nset by a much smaller number of â€œnewâ€ variables, named as principal components  (Gillies,D., ,2005) .  For \\ne x a m p l e ,  i n  ï¬ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \\nmeasured in the X -Y coordinate system.  \\nRed Circles:            ğ‘¥1,ğ‘¥2,â€¦ ğ‘¥ğ‘›                                        ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¥ \\nBlue Circle            ğ‘¦1,ğ‘¦2,â€¦,ğ‘¦ğ‘›                                       ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¦ \\nThe principal directi on in which the data varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \\na given set of data, principal component analysis ï¬nds the axis system deï¬ned by the principal directions o\\nf  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ï¬ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \\ncomponents.  \\n \\n  \\n \\nFigure 1  Figure 2  \\nIf the variation in a data set is caused by some natural property, or is caused by random experimental \\nerror, then we may expect it to be normally distributed. In this case we show the nominal extent of the \\nnormal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \\ndata points that are thought of as belonging to a class. It is drawn at a distance beyond which the \\nprobability of a point belonging to the class is low, and can be thought of as a class boundary.  \\nIf the variation in the data is caused by some other relationship, then PCA gives us a way of \\nreducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \\nin ï¬gure2. As in ï¬gure1 the principal direction in which the data varies is shown by the U axis and the \\nsecondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \\nmay assume, for example, that they are only non -zero because of experimental noise  or measurement \\n  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \\nhave reduced the  dimensionality of the problem by 1.  \\nIn computational terms the principal components are found by calculating the eigenvectors and \\neigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \\nthe co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \\nvariation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \\nvariation and so on. In the other word, eigenvalues are the variance of principal components. The first \\neigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \\nsecond principal component and so on (Gillies,D., 2005) . \\nThe eigenvalues of A, ğ‘› Ã—ğ‘› matrix, are defined as the roots of:  \\ndet(ğ´âˆ’Î»)=|ğ´âˆ’Î»I|=0 (5) \\nLet Î» be an eigenvalue of A. Then there exists a vector ğ‘¥ such that:  \\nğ´ğ‘¥=Î»ğ‘¥ (6) \\nThe vector ğ‘¥  is called an eigenvector of A, associated with the eigenvalue Î» . Notice that there is no \\nunique solution for ğ‘¥  in the above equation. It is a direction vector only and can be scaled to any \\nmagnitude. To find a numerical solution for ğ‘¥ we need to set one of its elements to an arbitrary value, say \\n1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \\nwe repeat the process with another element. Ordinarily we normalize the final values so that ğ‘¥  has length \\none, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \\nSuppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \\nğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ğ‘¥3=ğœ†3ğ‘¥3                 (7) \\nPutting the eigenvectors as the columns of a matrix gives:  \\nğ´[ğ‘¥1ğ‘¥2ğ‘¥3]=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½[ğ‘¥1ğ‘¥2ğ‘¥3]   (8) \\nWriting:  \\nğœ‘=[ğ‘¥1ğ‘¥2ğ‘¥3]                     ÊŒ=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½  (9) \\nGives us the matrix equation:  \\nğ´ğœ‘=ÊŒğœ‘  (10) \\nWe normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \\nğœ‘ğœ‘ğ‘‡=ğœ‘ğ‘‡ğœ‘=ğ¼  (5) \\nThis means that:  \\nğœ‘ğ‘‡ğ´ğœ‘=ÊŒ  (6) \\nAnd:  \\nğ´=ğœ‘ÊŒğœ‘ğ‘‡  (7) \\nN o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Î£  b e  a  ğ‘›Ã—ğ‘› \\ncovariance matrix. There is an orthogonal  ğ‘› Ã—ğ‘› matrix ğœ‘ whose columns are eigenvectors of Î£ and a \\ndiagonal matrix ÊŒ  whose diagonal elements are the eigenvalues of Î£, such that:  \\nğœ‘Î£ğœ‘ğ‘‡=ÊŒ  (8) \\nWe can look on the matrix of eigenvectors ğœ‘ as a linear transformation which, in the example of figure1 \\ntransforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by ğœ‘  transforms the data points into a data set where the variabl es are \\nuncorrelated. The correlation matrix of the data in the new coordinate system is ÊŒ which has zeros in all \\nthe off diagonal elements.  \\nPrincipal component analysis is appropriate when you have obtained measures on a number of \\nobserved variables and wi sh to develop a smaller number of artificial variables (called principal \\ncomponents) that will account for most of the variance in the observed variables.  Some limitations of PCA  \\n(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \\nonly consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \\nof PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \\nmatrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \\nthe original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \\nvariance.  (5) PCA is not scale invariant.  \\n \\nImplementation  \\nIn this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables displayed in Figure 3 . This is an \\nappropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \\nFigure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \\n1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \\nwill find the correlation between the two correspond ing variables. For example, where the row for \\nvariable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \\ncorrelation between variables 1 and 2 is 0.14 . \\nBased on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \\n(ğœŒ25=0.76  ,ğœŒ26=0.66 ,ğœŒ56=0.56 ) . \\nVariable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (ğœŒ\\n42=âˆ’0.73  ,ğœŒ45=âˆ’0.59 ,ğœŒ46=âˆ’0.46 ). \\nVariable 3 also shows negative correlation with variab le 2, 5 and 6 \\n (ğœŒ32=âˆ’0.53  ,ğœŒ35=âˆ’0.38 , ğœŒ36=âˆ’0.49 ). \\nHowever, variable 1 has no correlation with the other variables. When the correlation between two \\nvariables is less than 0.2 ( |ğœŒ|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \\nLetâ€™s reorder variables based on their correlations to each other  and have a new correlation matrix for \\nreordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \\nsecond and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \\nplaces at the last order.  \\n1 Ni(V1)  Fe(V2)  \\n2 Fe(V2)  Co(V5)  \\n3 SiO2(V3)  Al2O3(V6)  \\n4 MgO(V4)  MgO(V4)  \\n5 Co(V5)  SiO2(V3)  \\n6 Al2O3(V6)  Ni(V1)  \\nTable1 : Reordering variables based on their correlation  \\n \\n \\nFigure 3 : Correlation matrix for six different variables       \\n Figure 4: Correlation matrix based on reorder ed variables  \\nPaper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \\ngroups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \\ngroup, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \\nno correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \\nessence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \\nwith many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \\nsmaller set of artificial variables which called dimension reduction.  \\n \\nFigure 5: Three distinct groups of variables  \\n \\nEigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \\nare reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \\nFe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \\nSiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \\nMgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \\nCo(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \\nAl2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \\nTable 2: Eigenvalues and eigenvectors for correlation matrix  \\n Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative  eigenvalues  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nTable 3: Reordered eigenvalues and eigenvectors for correlation matrix  \\nCumulative eigenvalue curve c(x) is defined to be:  \\nğ¶(ğ‘¥)=âˆ‘Î»ğ‘–ğ‘¥\\nğ‘–\\nâˆ‘Î»ğ‘–ğ‘\\nğ‘–=1Ã—100  (15) \\nSecond group  \\nFirst  group  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 6  \\nFigure 6: Cumulative eigenvalue VS Order of eigenvalues  \\nThe interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \\nmaintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \\nA feature transformation that, for instance, retains 50 percent of the original information (variance) of the \\ninput data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \\ninformation can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\nğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğºğ‘Ÿğ‘’ğ‘’ğ‘›\\n|ğœŒ|<0.2        ğ‘›ğ‘œ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›           â†’ğºğ‘Ÿğ‘ğ‘¦\\nâˆ’1â‰¤ğœŒâ‰¤âˆ’0.2ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğ‘‚ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ \\nThen, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \\neigenvectors ( Î»=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \\nvariable 3 and 4 and de -correlation for variable 2  as expected . \\n  Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative eigenvalues/Number of variables  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nTable 4 : Reordering eigenvectors based on variable correlation (Table1)  \\nSummary and Future work  \\nThe c orrelation matrix summarizes correlation among several variables can describe statistical \\ndependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \\ncalculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \\ncalculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \\nmethods such as Multi -Dimensi onal scaling and factorization will be described.  \\n \\nReferences  \\nBabak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \\nCorrelated Data.  \\nGillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \\nIzenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 3 4 5 6Eigenvalues cummulative  \\nOrder of Eigenvalues  '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_question_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunk_question_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_question_gen = [Document(page_content=text) for text in chunk_question_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_question_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(model_name = \"gpt-3.5-turbo\",\n",
    "                                     chunk_size = 1000,\n",
    "                                     chunk_overlap = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ans_gen = splitter_ans_gen.split_documents(\n",
    "    document_question_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \\nMaryam Hadavand -Siri and Clayton V.Deutsch  \\n \\nThe c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \\ndata sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \\npredict spatial distribution and dependency between variables, correlation matrix is  a key element  to \\ndescribe this dependency . The principal directions of data set variance are defined by pri ncipal \\ncomponents. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \\neigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \\n Introduction  \\nDependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set ğ‘‹,ğ‘Œ defined as:  \\nğœŒğ‘‹ğ‘Œ=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)= ğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)\\nğœğ‘‹ğœğ‘Œ=ğ¸[(ğ‘‹âˆ’ğœ‡ğ‘‹)(ğ‘Œâˆ’ğœ‡ğ‘Œ]\\nğœğ‘‹ğœğ‘Œ (1) \\nğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)=1\\nğ‘ï¿½ (ğ‘‹ğ‘–ğ‘›\\nğ‘–=1âˆ’ğœ‡ğ‘‹)(ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ) (2) \\nğœğ‘‹=ï¿½âˆ‘ (ğ‘‹ğ‘–âˆ’ğœ‡ğ‘‹)ğ‘›\\nğ‘–=1\\nğ‘›          ğœğ‘Œ=ï¿½âˆ‘ (ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ)ğ‘›\\nğ‘–=1\\nğ‘› (3) \\nWhere ğœŒ,ğ¸ ğ‘ğ‘›ğ‘‘  ğ¶ğ‘œğ‘£ , are correlation, expected value and covariance operator respectively, ğœ‡   is the \\nmean, ğœ is standard deviation  and ğ‘› is number of variables.  The correlation is +1 in the case of a perfect \\npositive linear relationship, âˆ’1 in th e case of a perfect negative linear relationship,  and some value \\nbetween âˆ’1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \\nit approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \\neither âˆ’1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \\nsummarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \\nbetween different data sets and are symmetric as ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘Œ,ğ‘‹). Correlation matrices must be \\npositive semi definite. It means for all Non -zero column vector  ğ‘, ğ‘\\nğ‘‡ğœŒğ‘>0 (ğ‘ğ‘‡ is the transpose of ğ‘ and  \\nğœŒ is correlation matrix).  \\nThe subject of multivariate analysis deals with the statistical analysis of the data collected on \\nmore than one variable. These variables may be correlated with each other, and their statistical \\ndependence is often taken into account when analyzing such data. Correlation matrix is a key element to \\nexplain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \\nwhich is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \\ndata.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, ğ‘‹â‰ 0 is a vector in ï¿½'),\n",
       " Document(page_content=' we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \\n \\nEigenvalues and eigenvectors : \\nPrincipal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \\ncovariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \\nsquare matrix of size n, ğ‘‹â‰ 0 is a vector in ğ¶\\nğ‘›, and Î»  is a scalar in C. Then ğ‘‹ is an eigenvector of A with \\neigenvalue Î», if ğ´ğ‘‹=Î»X.  The eigenvalues of a matrix  A are precisely the solutions Î» to the characteristic \\nequation ( I is identity matrix).  \\ndet(ğ´âˆ’Î»I)=0 \\ndet(ğ´âˆ’Î»I)=detï¿½ï¿½ğ‘11 0\\n0  ğ‘22â‹¯   0\\n0   0\\nâ‹®   â‹®\\n0  0â‹±  0\\n  0   ğ‘ğ‘›ğ‘›ï¿½âˆ’Î»ï¿½10\\n01â€¦ 0\\nâ€¦ 0\\nâ‹®â‹®\\n00â‹±0\\n01ï¿½ï¿½  \\n \\n \\n \\n(4) Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 2                        =ğ‘‘ğ‘’ğ‘¡ï¿½ğ‘11âˆ’Î» 0\\n0ğ‘22âˆ’Î»â‹¯           0\\nâ€¦           0\\nâ‹®            â‹®\\n0             0  â‹±           0\\n0   ğ‘ğ‘›ğ‘›âˆ’Î»ï¿½ \\n                      =(ğ‘11âˆ’Î»)(ğ‘22âˆ’Î»)â€¦(ğ‘ğ‘›ğ‘›âˆ’Î»)=0 \\n \\nThe solutions to this equation are the eigenvalues Î»ğ‘–=ğ‘ğ‘–ğ‘– (i=1,2,â€¦n).   When we deal with large size of \\nmatrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \\ntwo classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \\nPartial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \\napproximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \\n(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \\non PCA method. MDS and factorization will be covered in future works.  \\n \\nPrincipal Co mponent Analysis (PCA) : \\nPrincipal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \\ncovariance structure of a set of variables. In particular it allows us to identify the principal directions in \\nwhich the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \\nset by a much smaller number of â€œnewâ€ variables, named as principal components  (Gillies,D., ,2005) .  For \\ne x a m p l e ,  i n  ï¬ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \\nmeasured in the X -Y coordinate system.  \\nRed Circles:            ğ‘¥1,ğ‘¥2,â€¦ ğ‘¥ğ‘›                                        ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¥ \\nBlue Circle            ğ‘¦1,ğ‘¦2,â€¦,ğ‘¦ğ‘›                                       ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¦ \\nThe principal directi on in which the data varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and'),\n",
       " Document(page_content=' varies is shown by the U axis and the second most important \\ndirection is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \\ngives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \\nvalue, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \\na given set of data, principal component analysis ï¬nds the axis system deï¬ned by the principal directions o\\nf  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ï¬ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \\ncomponents.  \\n \\n  \\n \\nFigure 1  Figure 2  \\nIf the variation in a data set is caused by some natural property, or is caused by random experimental \\nerror, then we may expect it to be normally distributed. In this case we show the nominal extent of the \\nnormal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \\ndata points that are thought of as belonging to a class. It is drawn at a distance beyond which the \\nprobability of a point belonging to the class is low, and can be thought of as a class boundary.  \\nIf the variation in the data is caused by some other relationship, then PCA gives us a way of \\nreducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \\nin ï¬gure2. As in ï¬gure1 the principal direction in which the data varies is shown by the U axis and the \\nsecondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \\nmay assume, for example, that they are only non -zero because of experimental noise  or measurement \\n  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \\nhave reduced the  dimensionality of the problem by 1.  \\nIn computational terms the principal components are found by calculating the eigenvectors and \\neigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \\nthe co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \\nvariation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \\nvariation and so on. In the other word, eigenvalues are the variance of principal components. The first \\neigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \\nsecond principal component and so on (Gillies,D., 2005) . \\nThe eigenvalues of A, ğ‘› Ã—ğ‘› matrix, are defined as the roots of:  \\ndet(ğ´âˆ’Î»)=|ğ´âˆ’Î»I|=0 (5) \\nLet Î» be an eigenvalue of A. Then there exists a vector ğ‘¥ such that:  \\nğ´ğ‘¥=Î»ğ‘¥ (6) \\nThe vector ğ‘¥  is called an eigenvector of A, associated with the eigenvalue Î» . Notice that there is no \\nunique solution for ğ‘¥  in the above equation. It is a direction vector only and can be scaled to any \\nmagnitude. To find a numerical solution for ğ‘¥ we need to set one of its elements to an arbitrary value, say \\n1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \\nwe repeat the process with another element. Ordinarily we normalize the final values so that ğ‘¥  has length \\none, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \\nSuppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \\nğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ï¿½'),\n",
       " Document(page_content=' length \\none, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \\nSuppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \\nğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ğ‘¥3=ğœ†3ğ‘¥3                 (7) \\nPutting the eigenvectors as the columns of a matrix gives:  \\nğ´[ğ‘¥1ğ‘¥2ğ‘¥3]=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½[ğ‘¥1ğ‘¥2ğ‘¥3]   (8) \\nWriting:  \\nğœ‘=[ğ‘¥1ğ‘¥2ğ‘¥3]                     ÊŒ=ï¿½Î»1 0 0\\n0Î»2 0\\n0 0Î»3ï¿½  (9) \\nGives us the matrix equation:  \\nğ´ğœ‘=ÊŒğœ‘  (10) \\nWe normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \\nğœ‘ğœ‘ğ‘‡=ğœ‘ğ‘‡ğœ‘=ğ¼  (5) \\nThis means that:  \\nğœ‘ğ‘‡ğ´ğœ‘=ÊŒ  (6) \\nAnd:  \\nğ´=ğœ‘ÊŒğœ‘ğ‘‡  (7) \\nN o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Î£  b e  a  ğ‘›Ã—ğ‘› \\ncovariance matrix. There is an orthogonal  ğ‘› Ã—ğ‘› matrix ğœ‘ whose columns are eigenvectors of Î£ and a \\ndiagonal matrix ÊŒ  whose diagonal elements are the eigenvalues of Î£, such that:  \\nğœ‘Î£ğœ‘ğ‘‡=ÊŒ  (8) \\nWe can look on the matrix of eigenvectors ğœ‘ as a linear transformation which, in the example of figure1 \\ntransforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by ğœ‘  transforms the data points into a data set where the variabl es are \\nuncorrelated. The correlation matrix of the data in the new coordinate system is ÊŒ which has zeros in all \\nthe off diagonal elements.  \\nPrincipal component analysis is appropriate when you have obtained measures on a number of \\nobserved variables and wi sh to develop a smaller number of artificial variables (called principal \\ncomponents) that will account for most of the variance in the observed variables.  Some limitations of PCA  \\n(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \\nonly consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \\nof PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \\nmatrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \\nthe original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \\nvariance.  (5) PCA is not scale invariant.  \\n \\nImplementation  \\nIn this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables'),\n",
       " Document(page_content=' \\neigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \\n(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \\nCorrelation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \\nStandard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \\nCalculated corre lation matrix for six different variables displayed in Figure 3 . This is an \\nappropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \\nFigure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \\n1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \\nwill find the correlation between the two correspond ing variables. For example, where the row for \\nvariable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \\ncorrelation between variables 1 and 2 is 0.14 . \\nBased on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \\n(ğœŒ25=0.76  ,ğœŒ26=0.66 ,ğœŒ56=0.56 ) . \\nVariable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (ğœŒ\\n42=âˆ’0.73  ,ğœŒ45=âˆ’0.59 ,ğœŒ46=âˆ’0.46 ). \\nVariable 3 also shows negative correlation with variab le 2, 5 and 6 \\n (ğœŒ32=âˆ’0.53  ,ğœŒ35=âˆ’0.38 , ğœŒ36=âˆ’0.49 ). \\nHowever, variable 1 has no correlation with the other variables. When the correlation between two \\nvariables is less than 0.2 ( |ğœŒ|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \\nLetâ€™s reorder variables based on their correlations to each other  and have a new correlation matrix for \\nreordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \\nsecond and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \\nplaces at the last order.  \\n1 Ni(V1)  Fe(V2)  \\n2 Fe(V2)  Co(V5)  \\n3 SiO2(V3)  Al2O3(V6)  \\n4 MgO(V4)  MgO(V4)  \\n5 Co(V5)  SiO2(V3)  \\n6 Al2O3(V6)  Ni(V1)  \\nTable1 : Reordering variables based on their correlation  \\n \\n \\nFigure 3 : Correlation matrix for six different variables       \\n Figure 4: Correlation matrix based on reorder ed variables  \\nPaper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \\ngroups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \\ngroup, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \\nno correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \\nessence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \\nwith many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \\nsmaller set of artificial variables which called dimension reduction.  \\n \\nFigure 5: Three distinct groups of variables  \\n \\nEigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \\nare reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732 '),\n",
       " Document(page_content=' magnitudes as the first eigenvector with largest eigenvalue is the direction \\nof greatest principal component and so on (Table 3).  \\n  Eigenvalues  \\n0.009  3.1376  0.8583  1.2835  0.2816  0.43  \\nEigenvector  \\nNi(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \\nFe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \\nSiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \\nMgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \\nCo(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \\nAl2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \\nTable 2: Eigenvalues and eigenvectors for correlation matrix  \\n Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative  eigenvalues  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nTable 3: Reordered eigenvalues and eigenvectors for correlation matrix  \\nCumulative eigenvalue curve c(x) is defined to be:  \\nğ¶(ğ‘¥)=âˆ‘Î»ğ‘–ğ‘¥\\nğ‘–\\nâˆ‘Î»ğ‘–ğ‘\\nğ‘–=1Ã—100  (15) \\nSecond group  \\nFirst  group  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \\n408- 6  \\nFigure 6: Cumulative eigenvalue VS Order of eigenvalues  \\nThe interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \\nmaintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \\nA feature transformation that, for instance, retains 50 percent of the original information (variance) of the \\ninput data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \\ninformation can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\nğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿï¿½'),\n",
       " Document(page_content='ues (C(x) =9 5).  If the data are  independent  then \\ncumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \\nğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğºğ‘Ÿğ‘’ğ‘’ğ‘›\\n|ğœŒ|<0.2        ğ‘›ğ‘œ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›           â†’ğºğ‘Ÿğ‘ğ‘¦\\nâˆ’1â‰¤ğœŒâ‰¤âˆ’0.2ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğ‘‚ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ \\nThen, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \\neigenvectors ( Î»=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \\nvariable 3 and 4 and de -correlation for variable 2  as expected . \\n  Re-ordered eigenvalues  \\n3.1376  1.2835  0.8583  0.43  0.2816  0.009  \\nCumulative eigenvalues/Number of variables  \\n0.52293  0.73685  0.8799  0.95157  0.9985  1 \\nEigenvectors  \\nFe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \\nCo(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \\nAl2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \\nMgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \\nSiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \\nNi(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \\nTable 4 : Reordering eigenvectors based on variable correlation (Table1)  \\nSummary and Future work  \\nThe c orrelation matrix summarizes correlation among several variables can describe statistical \\ndependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \\ncalculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \\ncalculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \\nmethods such as Multi -Dimensi onal scaling and factorization will be described.  \\n \\nReferences  \\nBabak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \\nCorrelated Data.  \\nGillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \\nIzenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 '),\n",
       " Document(page_content=', Modern Multivariate Statistical Techniques, pp 597 -606  \\nKumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \\nPanhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \\nWickelmaier, F.,2003, An Introduction to MDS.  020406080100\\n0 1 2 3 4 5 6Eigenvalues cummulative  \\nOrder of Eigenvalues  ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_ans_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_ans_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ankit\\miniconda3\\envs\\interview\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "WARNING! frequency_penalty is not default parameter.\n",
      "                    frequency_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that frequency_penalty is what you intended.\n",
      "WARNING! presence_penalty is not default parameter.\n",
      "                    presence_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that presence_penalty is what you intended.\n"
     ]
    }
   ],
   "source": [
    "# Define the LLM pipeline\n",
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.3,\n",
    "    max_tokens=1000,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an expert in creating exam questions based on coding materials and documentation.\n",
    "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
    "You will achieve this by asking questions about the text provided below:\n",
    "\n",
    "---------\n",
    "{text}\n",
    "---------\n",
    "\n",
    "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
    "Ensure that all important information is covered without omitting any key details.\n",
    "\n",
    "Questions:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_questions = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the refine template\n",
    "refine_template = \"\"\"\n",
    "You are an expert in creating exam questions based on coding materials and documentation.\n",
    "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
    "You will achieve this by asking questions about the text provided below:\n",
    "\n",
    "---------\n",
    "{text}\n",
    "---------\n",
    "\n",
    "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
    "Ensure that all important information is covered without omitting any key details.\n",
    "\n",
    "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
    "\n",
    "- Multiple-choice questions\n",
    "- True/false statements\n",
    "- Short answer questions\n",
    "- Problem-solving scenarios\n",
    "- Code debugging tasks\n",
    "\n",
    "Refined Questions:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_prompt_questions = PromptTemplate(\n",
    "    template=refine_template,\n",
    "    input_variables=[\"existing_answers\", \"text\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# Load the summarize chain\n",
    "ques_gen_chain = load_summarize_chain(\n",
    "    llm=llm_ques_gen_pipeline,\n",
    "    chain_type=\"refine\",\n",
    "    verbose=True,\n",
    "    question_prompt=prompt_questions,\n",
    "    refine_prompt=refined_prompt_questions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ankit\\miniconda3\\envs\\interview\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      "Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 1 Some Thoughts on Und erstanding  Correlation Matrices  \n",
      "Maryam Hadavand -Siri and Clayton V.Deutsch  \n",
      " \n",
      "The c orrelation matrix is a positive semi definite matrix that  describes  the dependen cy between different \n",
      "data sets. In case of multi variables  mode or dealing with many secondary variables which is hard to \n",
      "predict spatial distribution and dependency between variables, correlation matrix is  a key element  to \n",
      "describe this dependency . The principal directions of data set variance are defined by pri ncipal \n",
      "components. Principal Component Analysis (PCA) is a statistical procedure to calculate eigenvalues  and \n",
      "eigenvectors of correlation matrix  which are principal component of data set , by dimension reduction.   \n",
      " Introduction  \n",
      "Dependency refers to any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationship involving dependence. Correlation between two set of data set ğ‘‹,ğ‘Œ defined as:  \n",
      "ğœŒğ‘‹ğ‘Œ=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)= ğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)\n",
      "ğœğ‘‹ğœğ‘Œ=ğ¸[(ğ‘‹âˆ’ğœ‡ğ‘‹)(ğ‘Œâˆ’ğœ‡ğ‘Œ]\n",
      "ğœğ‘‹ğœğ‘Œ (1) \n",
      "ğ¶ğ‘œğ‘£ (ğ‘‹,ğ‘Œ)=1\n",
      "ğ‘ï¿½ (ğ‘‹ğ‘–ğ‘›\n",
      "ğ‘–=1âˆ’ğœ‡ğ‘‹)(ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ) (2) \n",
      "ğœğ‘‹=ï¿½âˆ‘ (ğ‘‹ğ‘–âˆ’ğœ‡ğ‘‹)ğ‘›\n",
      "ğ‘–=1\n",
      "ğ‘›          ğœğ‘Œ=ï¿½âˆ‘ (ğ‘Œğ‘–âˆ’ğœ‡ğ‘Œ)ğ‘›\n",
      "ğ‘–=1\n",
      "ğ‘› (3) \n",
      "Where ğœŒ,ğ¸ ğ‘ğ‘›ğ‘‘  ğ¶ğ‘œğ‘£ , are correlation, expected value and covariance operator respectively, ğœ‡   is the \n",
      "mean, ğœ is standard deviation  and ğ‘› is number of variables.  The correlation is +1 in the case of a perfect \n",
      "positive linear relationship, âˆ’1 in th e case of a perfect negative linear relationship,  and some value \n",
      "between âˆ’1 and 1 in all other cases, indicating the degree of linear dependence  between the variables. As \n",
      "it approaches zero there is less of a relationship (closer to uncorrelated). The clos er the coefficient is to \n",
      "either âˆ’1 or 1, the stronger the correlation between the variables. The correlation cannot exceed 1 in absolute value. When correlations among several variables are computed, they are typically \n",
      "summarized in the form of a correlati on matrix. Correlation matrices are built to describe the dependency \n",
      "between different data sets and are symmetric as ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘‹,ğ‘Œ)=ğ¶ğ‘œğ‘Ÿğ‘Ÿ (ğ‘Œ,ğ‘‹). Correlation matrices must be \n",
      "positive semi definite. It means for all Non -zero column vector  ğ‘, ğ‘\n",
      "ğ‘‡ğœŒğ‘>0 (ğ‘ğ‘‡ is the transpose of ğ‘ and  \n",
      "ğœŒ is correlation matrix).  \n",
      "The subject of multivariate analysis deals with the statistical analysis of the data collected on \n",
      "more than one variable. These variables may be correlated with each other, and their statistical \n",
      "dependence is often taken into account when analyzing such data. Correlation matrix is a key element to \n",
      "explain and apply this dependency in multi variable mode.  In reservoir estimation the primary well data, \n",
      "which is expensive to obtain by drilling is p redicted using the easy and cheap obtaining secondary seismic \n",
      "data.  Correlation matrix can be useful for spatial prediction and dimension reduction when we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \n",
      " \n",
      "Eigenvalues and eigenvectors : \n",
      "Principal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \n",
      "covariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \n",
      "square matrix of size n, ğ‘‹â‰ 0 is a vector in ï¿½\n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      " we are dealing with many secondary variables  (Kumar , A. and Deutsch, C.V. , 2009, CCG annual report ). \n",
      " \n",
      "Eigenvalues and eigenvectors : \n",
      "Principal component of a data set are found by calculating the eigenvalues and eigenvectors of the data \n",
      "covariance matrix. In fact, eigenvalues are the variance of principal components .  Suppose that A is a \n",
      "square matrix of size n, ğ‘‹â‰ 0 is a vector in ğ¶\n",
      "ğ‘›, and Î»  is a scalar in C. Then ğ‘‹ is an eigenvector of A with \n",
      "eigenvalue Î», if ğ´ğ‘‹=Î»X.  The eigenvalues of a matrix  A are precisely the solutions Î» to the characteristic \n",
      "equation ( I is identity matrix).  \n",
      "det(ğ´âˆ’Î»I)=0 \n",
      "det(ğ´âˆ’Î»I)=detï¿½ï¿½ğ‘11 0\n",
      "0  ğ‘22â‹¯   0\n",
      "0   0\n",
      "â‹®   â‹®\n",
      "0  0â‹±  0\n",
      "  0   ğ‘ğ‘›ğ‘›ï¿½âˆ’Î»ï¿½10\n",
      "01â€¦ 0\n",
      "â€¦ 0\n",
      "â‹®â‹®\n",
      "00â‹±0\n",
      "01ï¿½ï¿½  \n",
      " \n",
      " \n",
      " \n",
      "(4) Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 2                        =ğ‘‘ğ‘’ğ‘¡ï¿½ğ‘11âˆ’Î» 0\n",
      "0ğ‘22âˆ’Î»â‹¯           0\n",
      "â€¦           0\n",
      "â‹®            â‹®\n",
      "0             0  â‹±           0\n",
      "0   ğ‘ğ‘›ğ‘›âˆ’Î»ï¿½ \n",
      "                      =(ğ‘11âˆ’Î»)(ğ‘22âˆ’Î»)â€¦(ğ‘ğ‘›ğ‘›âˆ’Î»)=0 \n",
      " \n",
      "The solutions to this equation are the eigenvalues Î»ğ‘–=ğ‘ğ‘–ğ‘– (i=1,2,â€¦n).   When we deal with large size of \n",
      "matrices, get the eigenvalues and eigenvectors from characteristic equation is not an easy job. There are \n",
      "two classes of numerical methods to calculate eigenvalues and eigenvectors  (Panhuis , P.H.M. , 2005) : (1) \n",
      "Partial methods  - computation of extrema eigenvalues such as power method, and (2) Global methods  - \n",
      "approximation of whole spectrum such as:  Principal Component Analysis (PCA) , Multi -Dimensional Scaling \n",
      "(MDS), and Factorization .  This report provides an introduction to global methods and specifically focuses \n",
      "on PCA method. MDS and factorization will be covered in future works.  \n",
      " \n",
      "Principal Co mponent Analysis (PCA) : \n",
      "Principal Component Analysis, or simply PCA, is a statistical procedure concerned with elucidating the \n",
      "covariance structure of a set of variables. In particular it allows us to identify the principal directions in \n",
      "which the data varies. Principal Component  Analysis (PCA) is a method of identifying the pattern of data \n",
      "set by a much smaller number of â€œnewâ€ variables, named as principal components  (Gillies,D., ,2005) .  For \n",
      "e x a m p l e ,  i n  ï¬ g u r e  1 ,  s u p p o s e  t h a t  t h e  s m a l l  c i r c l e s  r e p r e s e n t  a  t w o  v a r i a b l e  d a t a  s e t  w h i ch we have \n",
      "measured in the X -Y coordinate system.  \n",
      "Red Circles:            ğ‘¥1,ğ‘¥2,â€¦ ğ‘¥ğ‘›                                        ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¥ \n",
      "Blue Circle            ğ‘¦1,ğ‘¦2,â€¦,ğ‘¦ğ‘›                                       ğ‘€ğ‘’ğ‘ğ‘›=ğœ‡ğ‘¦ \n",
      "The principal directi on in which the data varies is shown by the U axis and the second most important \n",
      "direction is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \n",
      "gives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \n",
      "value, the data is de -correlated, meaning that the co -variance between the U and\n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      " varies is shown by the U axis and the second most important \n",
      "direction is the V axis orthogonal to it. If we place the [U,V] axis system at the mean of the data (ğœ‡ğ‘¥, ğœ‡ğ‘¦) it \n",
      "gives us a compact representation. If we transform each [X,Y ] coordinate into its corresponding [U,V] \n",
      "value, the data is de -correlated, meaning that the co -variance between the U and V variables is zero.  For \n",
      "a given set of data, principal component analysis ï¬nds the axis system deï¬ned by the principal directions o\n",
      "f  v a r i a n c e  ( i e  t h e  [ U , V ]  a x i s  s y s t e m  i n  ï¬ g u r e  1 )  .  T h e  d i r e c t i o n s  U  a n d  V  a r e  c a l l e d  t h e  p r i n c i p a l  \n",
      "components.  \n",
      " \n",
      "  \n",
      " \n",
      "Figure 1  Figure 2  \n",
      "If the variation in a data set is caused by some natural property, or is caused by random experimental \n",
      "error, then we may expect it to be normally distributed. In this case we show the nominal extent of the \n",
      "normal distribution by a hyper -ellipse (the two dimensional ellipse in Figure1). The hyper ellipse encloses \n",
      "data points that are thought of as belonging to a class. It is drawn at a distance beyond which the \n",
      "probability of a point belonging to the class is low, and can be thought of as a class boundary.  \n",
      "If the variation in the data is caused by some other relationship, then PCA gives us a way of \n",
      "reducing the dimensionali ty of a data set. Consider two variables that are nearly related linearly as shown \n",
      "in ï¬gure2. As in ï¬gure1 the principal direction in which the data varies is shown by the U axis and the \n",
      "secondary direction by the V axis. However in this case all the V coo rdinates are very close to zero. We \n",
      "may assume, for example, that they are only non -zero because of experimental noise  or measurement \n",
      "  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 3 error. Thus in the U -V axis system we can represent the data set by one variable U and discard V. Thus we \n",
      "have reduced the  dimensionality of the problem by 1.  \n",
      "In computational terms the principal components are found by calculating the eigenvectors and \n",
      "eigenvalues of the data covariance matrix. This process is equivalent to finding the axis system in which \n",
      "the co -variance mat rix is diagonal. The eigenvector with the largest eigenvalue is the direction of greatest \n",
      "variation, the one with the second largest eigenvalue is the (orthogonal) direction with the next highest \n",
      "variation and so on. In the other word, eigenvalues are the variance of principal components. The first \n",
      "eigenvalue is the variance of the first principal component; the second eigenvalue is the variance of the \n",
      "second principal component and so on (Gillies,D., 2005) . \n",
      "The eigenvalues of A, ğ‘› Ã—ğ‘› matrix, are defined as the roots of:  \n",
      "det(ğ´âˆ’Î»)=|ğ´âˆ’Î»I|=0 (5) \n",
      "Let Î» be an eigenvalue of A. Then there exists a vector ğ‘¥ such that:  \n",
      "ğ´ğ‘¥=Î»ğ‘¥ (6) \n",
      "The vector ğ‘¥  is called an eigenvector of A, associated with the eigenvalue Î» . Notice that there is no \n",
      "unique solution for ğ‘¥  in the above equation. It is a direction vector only and can be scaled to any \n",
      "magnitude. To find a numerical solution for ğ‘¥ we need to set one of its elements to an arbitrary value, say \n",
      "1, which gives us a set of simultaneous equations to solve for the other elements. If there is no solution \n",
      "we repeat the process with another element. Ordinarily we normalize the final values so that ğ‘¥  has length \n",
      "one, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \n",
      "Suppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \n",
      "ğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ï¿½\n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      " length \n",
      "one, that is ğ‘¥ .ğ‘¥ğ‘‡=1. \n",
      "Suppose we have a 3Ã—3 matrix A with eigenvectors ğ‘¥1,ğ‘¥2,ğ‘¥3 and eigenvalues Î»1,Î»2,Î»3 so: \n",
      "ğ´ğ‘¥1=Î»1ğ‘¥1                  ğ´ğ‘¥2=ğœ†2ğ‘¥2                ğ´ğ‘¥3=ğœ†3ğ‘¥3                 (7) \n",
      "Putting the eigenvectors as the columns of a matrix gives:  \n",
      "ğ´[ğ‘¥1ğ‘¥2ğ‘¥3]=ï¿½Î»1 0 0\n",
      "0Î»2 0\n",
      "0 0Î»3ï¿½[ğ‘¥1ğ‘¥2ğ‘¥3]   (8) \n",
      "Writing:  \n",
      "ğœ‘=[ğ‘¥1ğ‘¥2ğ‘¥3]                     ÊŒ=ï¿½Î»1 0 0\n",
      "0Î»2 0\n",
      "0 0Î»3ï¿½  (9) \n",
      "Gives us the matrix equation:  \n",
      "ğ´ğœ‘=ÊŒğœ‘  (10) \n",
      "We normalized the eigenvectors to unit magnitude, and they are orthogonal, so:  \n",
      "ğœ‘ğœ‘ğ‘‡=ğœ‘ğ‘‡ğœ‘=ğ¼  (5) \n",
      "This means that:  \n",
      "ğœ‘ğ‘‡ğ´ğœ‘=ÊŒ  (6) \n",
      "And:  \n",
      "ğ´=ğœ‘ÊŒğœ‘ğ‘‡  (7) \n",
      "N o w  l e t  u s  c o n s i d e r  h o w  t h i s  a p p l i e s  t o  t h e  c o v a r i a n c e  m a t r i x  i n  t h e  P C A  p r o c e s s .  L e t  Î£  b e  a  ğ‘›Ã—ğ‘› \n",
      "covariance matrix. There is an orthogonal  ğ‘› Ã—ğ‘› matrix ğœ‘ whose columns are eigenvectors of Î£ and a \n",
      "diagonal matrix ÊŒ  whose diagonal elements are the eigenvalues of Î£, such that:  \n",
      "ğœ‘Î£ğœ‘ğ‘‡=ÊŒ  (8) \n",
      "We can look on the matrix of eigenvectors ğœ‘ as a linear transformation which, in the example of figure1 \n",
      "transforms data points in the [X, Y] axis system into the [U,V] axis system. In the general case the linear transformation given by ğœ‘  transforms the data points into a data set where the variabl es are \n",
      "uncorrelated. The correlation matrix of the data in the new coordinate system is ÊŒ which has zeros in all \n",
      "the off diagonal elements.  \n",
      "Principal component analysis is appropriate when you have obtained measures on a number of \n",
      "observed variables and wi sh to develop a smaller number of artificial variables (called principal \n",
      "components) that will account for most of the variance in the observed variables.  Some limitations of PCA  \n",
      "(Izenman, A. J.,2008) : (1) The directions with largest variance are assumed to be of most interest.  (2) We \n",
      "only consider orthogonal transformations (rotations) of the original variables. (Kernel PCA is an extension \n",
      "of PCA that allows non -linear mappings).  (3) PCA is based only on the mean vector and the covariance \n",
      "matrix of the da ta. Some distributions (e.g. multivariate normal) are completely characterized by this, but Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 4 others are not.  (4) Dimension reduction can only be achieved if the original variables were correlated. If \n",
      "the original variables were uncorrelated, PCA does nothin g, except for ordering them according to their \n",
      "variance.  (5) PCA is not scale invariant.  \n",
      " \n",
      "Implementation  \n",
      "In this study, a program written in fortran90 code used to calculate correlation matrix, eigenvalues and \n",
      "eigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \n",
      "(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \n",
      "Correlation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \n",
      "Standard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \n",
      "Calculated corre lation matrix for six different variables\n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      " \n",
      "eigenvectors for a data set with six variables. This code reads data set in ASCII format,  deletes null values \n",
      "(use full valued subset to delete -999)  and it calculate s Mean, Standard deviation, Covariance and \n",
      "Correlation matrix for data set. Then calculate eigenvalues and eigenvectors for the correlation matrix. \n",
      "Standard GSLIB convention (corrmat_plot)  is used  to plot correlation matrix.  \n",
      "Calculated corre lation matrix for six different variables displayed in Figure 3 . This is an \n",
      "appropriate opportunity to review just how a correlation matrix is interpreted. The  rows and columns of \n",
      "Figure  3 correspond to the six variables included in the analysis: Row 1 (and column 1) represents variable \n",
      "1, row 2 (and column 2) represents variable 2, and so forth. When a given row and column intersect, you \n",
      "will find the correlation between the two correspond ing variables. For example, where the row for \n",
      "variable 2 intersects with the column for variable 1, you find a correlation of 0.14 ; this means that the \n",
      "correlation between variables 1 and 2 is 0.14 . \n",
      "Based on Figure 3, variables 2, 5 and 6 show relatively s trong positive correlation with one another \n",
      "(ğœŒ25=0.76  ,ğœŒ26=0.66 ,ğœŒ56=0.56 ) . \n",
      "Variable 4 shows relatively strong negative correlation with variable 2, 5 and 6  (ğœŒ\n",
      "42=âˆ’0.73  ,ğœŒ45=âˆ’0.59 ,ğœŒ46=âˆ’0.46 ). \n",
      "Variable 3 also shows negative correlation with variab le 2, 5 and 6 \n",
      " (ğœŒ32=âˆ’0.53  ,ğœŒ35=âˆ’0.38 , ğœŒ36=âˆ’0.49 ). \n",
      "However, variable 1 has no correlation with the other variables. When the correlation between two \n",
      "variables is less than 0.2 ( |ğœŒ|<0.2) we assume they are uncorrelated (Babak , O. and Deutsch, C.V., 2008 ). \n",
      "Letâ€™s reorder variables based on their correlations to each other  and have a new correlation matrix for \n",
      "reordered variables.  Variable  2, 5 and 6 which have strong positive correlation to each other place in first, \n",
      "second and third orders respectively. Variable 4 and 3 which have negative correlation with variables 2, 5 and 6 place in fourth and fifth order respectively. Variable 1 has no correlation with the other vari ables \n",
      "places at the last order.  \n",
      "1 Ni(V1)  Fe(V2)  \n",
      "2 Fe(V2)  Co(V5)  \n",
      "3 SiO2(V3)  Al2O3(V6)  \n",
      "4 MgO(V4)  MgO(V4)  \n",
      "5 Co(V5)  SiO2(V3)  \n",
      "6 Al2O3(V6)  Ni(V1)  \n",
      "Table1 : Reordering variables based on their correlation  \n",
      " \n",
      " \n",
      "Figure 3 : Correlation matrix for six different variables       \n",
      " Figure 4: Correlation matrix based on reorder ed variables  \n",
      "Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 5 The new correlation matrix ( Figure 4) shows that the six variables seem to hang together in three distinct \n",
      "groups. First group, variable2,5 and 6 which they have strong positive correlation to each other. Second \n",
      "group, variable3 and 4 which they have negative correlations with group1. The last group is variable1 has \n",
      "no correlation with the rest of variables  (Figure 5). This is the redundancy of six variables to three.  In \n",
      "essence, this is what accomplished by correlation matrix. In multivariate analysis mode or when dealing \n",
      "with many secondary variables, correlation matrix allows you to reduce a set of observed variables into a \n",
      "smaller set of artificial variables which called dimension reduction.  \n",
      " \n",
      "Figure 5: Three distinct groups of variables  \n",
      " \n",
      "Eigenvalues and eigenvectors are calculated for this correlation matrix and displayed on Table 2. Then they \n",
      "are reordered based on their magnitudes as the first eigenvector with largest eigenvalue is the direction \n",
      "of greatest principal component and so on (Table 3).  \n",
      "  Eigenvalues  \n",
      "0.009  3.1376  0.8583  1.2835  0.2816  0.43  \n",
      "Eigenvector  \n",
      "Ni(V1)  -0.0553  0.1469  0.8668  0.4732 \n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      " magnitudes as the first eigenvector with largest eigenvalue is the direction \n",
      "of greatest principal component and so on (Table 3).  \n",
      "  Eigenvalues  \n",
      "0.009  3.1376  0.8583  1.2835  0.2816  0.43  \n",
      "Eigenvector  \n",
      "Ni(V1)  -0.0553  0.1469  0.8668  0.4732  0.0095  0.0025  \n",
      "Fe(V2)  -0.6259  0.5325  -0.083  -0.0748  -0.5228  -0.1971  \n",
      "SiO2(V3)  -0.4957  -0.3081  0.3746  -0.6546  0.2518  0.1645  \n",
      "MgO(V4)  -0.5912  -0.3942  -0.2838  0.5675  0.2962  -0.0691  \n",
      "Co(V5)  0.0025  0.4836  -0.0306  -0.1053  0.7021  -0.511  \n",
      "Al2O3(V6)  -0.0994  0.4589  -0.1411  0.0944  0.2872  0.8174  \n",
      "Table 2: Eigenvalues and eigenvectors for correlation matrix  \n",
      " Re-ordered eigenvalues  \n",
      "3.1376  1.2835  0.8583  0.43  0.2816  0.009  \n",
      "Cumulative  eigenvalues  \n",
      "0.52293  0.73685  0.8799  0.95157  0.9985  1 \n",
      "Eigenvectors  \n",
      "Ni(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \n",
      "Fe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \n",
      "SiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \n",
      "MgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \n",
      "Co(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \n",
      "Al2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \n",
      "Table 3: Reordered eigenvalues and eigenvectors for correlation matrix  \n",
      "Cumulative eigenvalue curve c(x) is defined to be:  \n",
      "ğ¶(ğ‘¥)=âˆ‘Î»ğ‘–ğ‘¥\n",
      "ğ‘–\n",
      "âˆ‘Î»ğ‘–ğ‘\n",
      "ğ‘–=1Ã—100  (15) \n",
      "Second group  \n",
      "First  group  Paper 408, CCG Annual Report 14 , 2012 (Â© 2012)  \n",
      "408- 6  \n",
      "Figure 6: Cumulative eigenvalue VS Order of eigenvalues  \n",
      "The interpretation of this curve, Figure 6 , is that the value C (x) represents the amount of information \n",
      "maintained in the input vectors if we project them onto the subspace spanned by the top x eigenvectors. \n",
      "A feature transformation that, for instance, retains 50 percent of the original information (variance) of the \n",
      "input data can be obtained by first eigenvalue(C(x) =50), and more than 9 5 percent of the original \n",
      "information can be obtained by first four eigenva lues (C(x) =9 5).  If the data are  independent  then \n",
      "cumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \n",
      "ğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿï¿½\n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      "ues (C(x) =9 5).  If the data are  independent  then \n",
      "cumulative curve, C(X), follow s the red line .  Consider the following classification  for the eigenvectors : \n",
      "ğ‘–ğ‘“ ï¿½0.2â‰¤ğœŒâ‰¤1  ğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğºğ‘Ÿğ‘’ğ‘’ğ‘›\n",
      "|ğœŒ|<0.2        ğ‘›ğ‘œ ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›           â†’ğºğ‘Ÿğ‘ğ‘¦\n",
      "âˆ’1â‰¤ğœŒâ‰¤âˆ’0.2ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’  ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â†’ğ‘‚ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ \n",
      "Then, it is easier to visualize and interpret eigenvectors based on variables correlation (Table 4 ). First \n",
      "eigenvectors ( Î»=3.1376) shows strong pos itive correlation for variable 2, 5 and 6, negative correlation for \n",
      "variable 3 and 4 and de -correlation for variable 2  as expected . \n",
      "  Re-ordered eigenvalues  \n",
      "3.1376  1.2835  0.8583  0.43  0.2816  0.009  \n",
      "Cumulative eigenvalues/Number of variables  \n",
      "0.52293  0.73685  0.8799  0.95157  0.9985  1 \n",
      "Eigenvectors  \n",
      "Fe(V2)  0.5325  -0.0748  -0.083  -0.1971  -0.5228  -0.6259  \n",
      "Co(V5)  0.4836  -0.1053  -0.0306  -0.511  0.7021  0.0025  \n",
      "Al2O3(V6)  0.4589  0.0944  -0.1411  0.8174  0.2872  -0.0994  \n",
      "MgO(V4)  -0.3942  0.5675  -0.2838  -0.0691  0.2962  -0.5912  \n",
      "SiO2(V3)  -0.3081  -0.6546  0.3746  0.1645  0.2518  -0.4957  \n",
      "Ni(V1)  0.1469  0.4732  0.8668  0.0025  0.0095  -0.0553  \n",
      "Table 4 : Reordering eigenvectors based on variable correlation (Table1)  \n",
      "Summary and Future work  \n",
      "The c orrelation matrix summarizes correlation among several variables can describe statistical \n",
      "dependency between them. When dealing with large size of matrices, it is difficult to calculate eigenvalues and eigenvectors from characteristic equation. There are s everal numerical methods to \n",
      "calculate eigenvalues and eigenvectors for large size of matrices. Principal component analysis (PCA) \n",
      "calculates eigenvalues and eigenvectors by dimension reduction.  In future work, other numerical \n",
      "methods such as Multi -Dimensi onal scaling and factorization will be described.  \n",
      " \n",
      "References  \n",
      "Babak, O., and Deutsch, C.,V., 2008, CCG annual report: Testing for the Multivariate Gaussian Distribution of Spatially \n",
      "Correlated Data.  \n",
      "Gillies, D., 2005, Lecture s and course materials: Principal Component Analysis.  \n",
      "Izenman, A.J., 2008, Modern Multivariate Statistical Techniques, pp 597 -606  \n",
      "Kumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \n",
      "Panhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \n",
      "Wickelmaier, F.,2003, An Introduction to MDS.  020406080100\n",
      "0 1 2 \n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert in creating exam questions based on coding materials and documentation.\n",
      "Your goal is to prepare engaging and comprehensive questions for exams, covering each concept thoroughly.\n",
      "You will achieve this by asking questions about the text provided below:\n",
      "\n",
      "---------\n",
      ", Modern Multivariate Statistical Techniques, pp 597 -606  \n",
      "Kumar, A., and Deutsch,C.V., 2009, CCG annual report : Optimal Correlation of Indefinite Correlation Matrices.  \n",
      "Panhuis , P.H.M.W., 2005, Iterative Techniques For Solving Eigenvalue problems.  \n",
      "Wickelmaier, F.,2003, An Introduction to MDS.  020406080100\n",
      "0 1 2 3 4 5 6Eigenvalues cummulative  \n",
      "Order of Eigenvalues  \n",
      "---------\n",
      "\n",
      "Create a set of questions that will effectively prepare coders and programmers for their tests.\n",
      "Ensure that all important information is covered without omitting any key details.\n",
      "\n",
      "Now, refine the questions to make them more challenging and thought-provoking. Consider including:\n",
      "\n",
      "- Multiple-choice questions\n",
      "- True/false statements\n",
      "- Short answer questions\n",
      "- Problem-solving scenarios\n",
      "- Code debugging tasks\n",
      "\n",
      "Refined Questions:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Multiple Choice: In the text provided, which author discusses Iterative Techniques For Solving Eigenvalue problems?\n",
      "   A. Kumar and Deutsch\n",
      "   B. Panhuis\n",
      "   C. Wickelmaier\n",
      "   D. None of the above\n",
      "\n",
      "2. True/False: The Eigenvalues cummulative table shows the order of eigenvalues in a decreasing fashion.\n",
      "   A. True\n",
      "   B. False\n",
      "\n",
      "3. Short Answer: Explain the concept of Optimal Correlation of Indefinite Correlation Matrices as discussed in the CCG annual report.\n",
      "\n",
      "4. Problem-Solving Scenario: Given a set of indefinite correlation matrices, how would you apply the techniques discussed in the text to find the optimal correlation?\n",
      "\n",
      "5. Code Debugging: Identify and correct any errors in the following code snippet related to Multivariate Statistical Techniques:\n",
      "\n",
      "```python\n",
      "def calculate_eigenvalues(matrix):\n",
      "    eigenvalues = np.linalg.eig(matrix)\n",
      "    return sum(eigenvalues)\n",
      "\n",
      "matrix = np.array([[1, 2], [3, 4]])\n",
      "result = calculate_eigenvalues(matrix)\n",
      "print(result)\n",
      "```\n",
      "\n",
      "6. Multiple Choice: Which author provides An Introduction to MDS in the text provided?\n",
      "   A. Kumar and Deutsch\n",
      "   B. Panhuis\n",
      "   C. Wickelmaier\n",
      "   D. None of the above\n",
      "\n",
      "7. True/False: Eigenvalues are used to represent the magnitude of variation in different directions in multivariate data.\n",
      "   A. True\n",
      "   B. False\n",
      "\n",
      "8. Short Answer: Describe the significance of eigenvalues in the context of multivariate statistical techniques and their applications.\n",
      "\n",
      "9. Problem-Solving Scenario: You are tasked with applying Multidimensional Scaling (MDS) to a dataset. How would you approach this task and what insights can be gained from the results?\n",
      "\n",
      "10. Code Debugging: Identify and correct any errors in the following code snippet related to solving eigenvalue problems iteratively:\n",
      "\n",
      "```python\n",
      "def iterative_eigenvalue_solver(matrix):\n",
      "    eigenvalues = []\n",
      "    for i in range(len(matrix)):\n",
      "        eigenvalues.append(np.linalg.eig(matrix[i]))\n",
      "    return eigenvalues\n",
      "\n",
      "matrix = np.array([[1, 2], [3, 4]])\n",
      "result = iterative_eigenvalue_solver(matrix)\n",
      "print(result)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "ques = ques_gen_chain.run(document_ans_gen)\n",
    "\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ankit\\miniconda3\\envs\\interview\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(document_ans_gen, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1fed2f2c640>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ans_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Multiple Choice: In the text provided, which author discusses Iterative Techniques For Solving Eigenvalue problems?\\n   A. Kumar and Deutsch\\n   B. Panhuis\\n   C. Wickelmaier\\n   D. None of the above\\n\\n2. True/False: The Eigenvalues cummulative table shows the order of eigenvalues in a decreasing fashion.\\n   A. True\\n   B. False\\n\\n3. Short Answer: Explain the concept of Optimal Correlation of Indefinite Correlation Matrices as discussed in the CCG annual report.\\n\\n4. Problem-Solving Scenario: Given a set of indefinite correlation matrices, how would you apply the techniques discussed in the text to find the optimal correlation?\\n\\n5. Code Debugging: Identify and correct any errors in the following code snippet related to Multivariate Statistical Techniques:\\n\\n```python\\ndef calculate_eigenvalues(matrix):\\n    eigenvalues = np.linalg.eig(matrix)\\n    return sum(eigenvalues)\\n\\nmatrix = np.array([[1, 2], [3, 4]])\\nresult = calculate_eigenvalues(matrix)\\nprint(result)\\n```\\n\\n6. Multiple Choice: Which author provides An Introduction to MDS in the text provided?\\n   A. Kumar and Deutsch\\n   B. Panhuis\\n   C. Wickelmaier\\n   D. None of the above\\n\\n7. True/False: Eigenvalues are used to represent the magnitude of variation in different directions in multivariate data.\\n   A. True\\n   B. False\\n\\n8. Short Answer: Describe the significance of eigenvalues in the context of multivariate statistical techniques and their applications.\\n\\n9. Problem-Solving Scenario: You are tasked with applying Multidimensional Scaling (MDS) to a dataset. How would you approach this task and what insights can be gained from the results?\\n\\n10. Code Debugging: Identify and correct any errors in the following code snippet related to solving eigenvalue problems iteratively:\\n\\n```python\\ndef iterative_eigenvalue_solver(matrix):\\n    eigenvalues = []\\n    for i in range(len(matrix)):\\n        eigenvalues.append(np.linalg.eig(matrix[i]))\\n    return eigenvalues\\n\\nmatrix = np.array([[1, 2], [3, 4]])\\nresult = iterative_eigenvalue_solver(matrix)\\nprint(result)\\n```'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = ques.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Multiple Choice: In the text provided, which author discusses Iterative Techniques For Solving Eigenvalue problems?',\n",
       " '   A. Kumar and Deutsch',\n",
       " '   B. Panhuis',\n",
       " '   C. Wickelmaier',\n",
       " '   D. None of the above',\n",
       " '',\n",
       " '2. True/False: The Eigenvalues cummulative table shows the order of eigenvalues in a decreasing fashion.',\n",
       " '   A. True',\n",
       " '   B. False',\n",
       " '',\n",
       " '3. Short Answer: Explain the concept of Optimal Correlation of Indefinite Correlation Matrices as discussed in the CCG annual report.',\n",
       " '',\n",
       " '4. Problem-Solving Scenario: Given a set of indefinite correlation matrices, how would you apply the techniques discussed in the text to find the optimal correlation?',\n",
       " '',\n",
       " '5. Code Debugging: Identify and correct any errors in the following code snippet related to Multivariate Statistical Techniques:',\n",
       " '',\n",
       " '```python',\n",
       " 'def calculate_eigenvalues(matrix):',\n",
       " '    eigenvalues = np.linalg.eig(matrix)',\n",
       " '    return sum(eigenvalues)',\n",
       " '',\n",
       " 'matrix = np.array([[1, 2], [3, 4]])',\n",
       " 'result = calculate_eigenvalues(matrix)',\n",
       " 'print(result)',\n",
       " '```',\n",
       " '',\n",
       " '6. Multiple Choice: Which author provides An Introduction to MDS in the text provided?',\n",
       " '   A. Kumar and Deutsch',\n",
       " '   B. Panhuis',\n",
       " '   C. Wickelmaier',\n",
       " '   D. None of the above',\n",
       " '',\n",
       " '7. True/False: Eigenvalues are used to represent the magnitude of variation in different directions in multivariate data.',\n",
       " '   A. True',\n",
       " '   B. False',\n",
       " '',\n",
       " '8. Short Answer: Describe the significance of eigenvalues in the context of multivariate statistical techniques and their applications.',\n",
       " '',\n",
       " '9. Problem-Solving Scenario: You are tasked with applying Multidimensional Scaling (MDS) to a dataset. How would you approach this task and what insights can be gained from the results?',\n",
       " '',\n",
       " '10. Code Debugging: Identify and correct any errors in the following code snippet related to solving eigenvalue problems iteratively:',\n",
       " '',\n",
       " '```python',\n",
       " 'def iterative_eigenvalue_solver(matrix):',\n",
       " '    eigenvalues = []',\n",
       " '    for i in range(len(matrix)):',\n",
       " '        eigenvalues.append(np.linalg.eig(matrix[i]))',\n",
       " '    return eigenvalues',\n",
       " '',\n",
       " 'matrix = np.array([[1, 2], [3, 4]])',\n",
       " 'result = iterative_eigenvalue_solver(matrix)',\n",
       " 'print(result)',\n",
       " '```']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=vector_store.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. Multiple Choice: In the text provided, which author discusses Iterative Techniques For Solving Eigenvalue problems?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Panhuis , P.H.M.W., 2005\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     A. Kumar and Deutsch\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  A. Kumar and Deutsch are authors who have contributed to the field of multivariate statistical techniques, specifically focusing on optimal correlation of indefinite correlation matrices and iterative techniques for solving eigenvalue problems.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     B. Panhuis\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have information specifically about B. Panhuis.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     C. Wickelmaier\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have information on C. Wickelmaier.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     D. None of the above\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm not sure what you mean by \"D. None of the above.\" Could you please provide more context or clarify your question?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking at information related to eigenvalues and eigenvectors, particularly in the context of Principal Component Analysis (PCA) and the calculation of covariance matrices. If you have a specific question or need clarification on any of these topics, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  2. True/False: The Eigenvalues cummulative table shows the order of eigenvalues in a decreasing fashion.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  True. The Eigenvalues cumulative table shows the order of eigenvalues in a decreasing fashion.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     A. True\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I'm not sure what specific statement or question you are referring to with \"A. True.\" Could you please provide more context or clarify your question?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     B. False\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I'm not sure what question or statement you are referring to with \"B. False.\" Could you please provide more context or clarify your question?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information on eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). Eigenvalues are the variance of principal components, and eigenvectors are the directions associated with these variances. PCA is a statistical procedure used to identify the principal directions in which data varies. If you have any specific questions or need further clarification, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  3. Short Answer: Explain the concept of Optimal Correlation of Indefinite Correlation Matrices as discussed in the CCG annual report.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The concept of Optimal Correlation of Indefinite Correlation Matrices, as discussed in the CCG annual report, refers to the process of determining the best possible correlation between different data sets represented by correlation matrices. In the context of multivariate statistical analysis, correlation matrices are used to describe the dependency between various variables. The optimal correlation aims to capture the most accurate and meaningful relationships between these variables, especially when dealing with complex data sets where spatial distribution and dependencies are hard to predict.\n",
      "\n",
      "By calculating eigenvalues and eigenvectors of correlation matrices, techniques like Principal Component Analysis (PCA) are employed to reduce the dimensions of the data and identify the principal components that explain the variance within the data set. This process helps in understanding the underlying structure of the data and finding the most significant correlations between variables. The goal of achieving optimal correlation is to enhance the interpretation and application of the data in various analytical processes, such as reservoir estimation or spatial prediction.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't have enough information to provide a response.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  4. Problem-Solving Scenario: Given a set of indefinite correlation matrices, how would you apply the techniques discussed in the text to find the optimal correlation?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  To find the optimal correlation of indefinite correlation matrices, you can apply Principal Component Analysis (PCA) techniques as discussed in the text. PCA helps in calculating the eigenvalues and eigenvectors of the correlation matrices, which are the principal components of the data set. By dimension reduction, PCA allows you to identify the principal directions of data set variance.\n",
      "\n",
      "Additionally, you can use numerical methods to calculate eigenvalues and eigenvectors for large matrices. Techniques like Multi-Dimensional Scaling and factorization can also be considered for further analysis.\n",
      "\n",
      "In summary, to find the optimal correlation of indefinite correlation matrices, you would utilize PCA for eigenvalue and eigenvector calculations, along with other numerical methods for large matrices, as discussed in the text.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you are looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). If you have a specific question or need clarification on any part of the information provided, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  5. Code Debugging: Identify and correct any errors in the following code snippet related to Multivariate Statistical Techniques:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I cannot see the actual code snippet you are referring to in the context provided. If you can provide the specific code snippet related to Multivariate Statistical Techniques that needs debugging, I'd be happy to help identify and correct any errors.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information on eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA) and how they are calculated. Eigenvalues are the variance of principal components, and eigenvectors are the directions associated with these eigenvalues. The eigenvalues and eigenvectors are calculated by finding the roots of the characteristic equation det(A - Î»I) = 0, where A is the matrix and Î» is the eigenvalue. The eigenvectors associated with these eigenvalues are found by solving the equation Ağ‘¥ = Î»ğ‘¥. If you have any specific questions or need further clarification, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  ```python\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but it seems like you haven't asked a question or provided any context for me to help you with. How can I assist you today?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  def calculate_eigenvalues(matrix):\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  To calculate the eigenvalues of a matrix, you can use numerical methods such as the power method for partial methods or Principal Component Analysis (PCA) for global methods. The eigenvalues of a matrix can be found by solving the characteristic equation det(A - Î»I) = 0, where A is the matrix, Î» is the eigenvalue, and I is the identity matrix. If you have the matrix in a specific format, you can apply the appropriate method to calculate the eigenvalues.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:      eigenvalues = np.linalg.eig(matrix)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The code you provided seems to be written in Python and is using the NumPy library to calculate the eigenvalues of a matrix. The function `np.linalg.eig(matrix)` in NumPy calculates the eigenvalues of the input matrix.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:      return sum(eigenvalues)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  To return the sum of the eigenvalues, you would need to add up all the individual eigenvalues. In the provided context, the eigenvalues are given as: 0.009, 3.1376, 0.8583, 1.2835, 0.2816, and 0.43. \n",
      "\n",
      "Summing these values:\n",
      "0.009 + 3.1376 + 0.8583 + 1.2835 + 0.2816 + 0.43 = 5.999\n",
      "\n",
      "Therefore, the sum of the eigenvalues is approximately 5.999.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). If you have a specific question or need clarification on any part of the information provided, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  matrix = np.array([[1, 2], [3, 4]])\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't know how to help with that.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  result = calculate_eigenvalues(matrix)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The function `calculate_eigenvalues(matrix)` would be used to calculate the eigenvalues of a given matrix. The eigenvalues are important in various mathematical operations, especially in Principal Component Analysis (PCA) where they represent the variance of the principal components.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  print(result)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't have access to the specific code or data you are referring to in order to print the result.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't see a specific question in your input. How can I assist you today?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). If you have a specific question or need clarification on any part of the text, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  6. Multiple Choice: Which author provides An Introduction to MDS in the text provided?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The author who provides An Introduction to MDS in the text provided is Wickelmaier, F. (2003).\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     A. Kumar and Deutsch\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  A. Kumar and Deutsch discuss the optimal correlation of indefinite correlation matrices in their CCG annual report. They also mention the difficulty of calculating eigenvalues and eigenvectors from characteristic equations when dealing with large matrices. Additionally, they introduce Principal Component Analysis (PCA) as a method for identifying the patterns of a dataset by reducing dimensions.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     B. Panhuis\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have information specifically about B. Panhuis.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     C. Wickelmaier\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have information on C. Wickelmaier.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     D. None of the above\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I couldn't find any options or questions in the context you provided. If you could provide more information or clarify your question, I'd be happy to help.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't have enough information to provide a response.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  7. True/False: Eigenvalues are used to represent the magnitude of variation in different directions in multivariate data.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  True.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     A. True\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but your message seems to be incomplete. Could you please provide more context or clarify your question so I can assist you better?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:     B. False\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I'm not sure what the statement \"B. False\" refers to in the context provided. If you could provide more information or context, I'd be happy to try and help.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA) and how they are calculated. The text explains that eigenvalues are the variance of principal components, and eigenvectors are the directions associated with these eigenvalues. The process of finding principal components involves calculating the eigenvectors and eigenvalues of the data covariance matrix. The eigenvalues are the roots of the characteristic equation det(A - Î»I) = 0, where A is the matrix and Î» is the eigenvalue. The eigenvectors associated with these eigenvalues are found by solving the equation Ağ‘¥ = Î»ğ‘¥. \n",
      "\n",
      "If you have any specific questions or need further clarification on this topic, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  8. Short Answer: Describe the significance of eigenvalues in the context of multivariate statistical techniques and their applications.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Eigenvalues play a significant role in multivariate statistical techniques, particularly in methods like Principal Component Analysis (PCA). In the context of multivariate analysis, eigenvalues represent the variance of the principal components of a dataset. By calculating the eigenvalues and eigenvectors of the data covariance matrix, we can identify the principal directions in which the data varies. These eigenvalues help in dimension reduction, allowing us to represent the data with a smaller number of variables while retaining the most important information. Eigenvalues provide insights into the structure and relationships within the dataset, aiding in data interpretation, visualization, and pattern recognition.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you are looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). If you have a specific question or need clarification on any part of the information provided, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  9. Problem-Solving Scenario: You are tasked with applying Multidimensional Scaling (MDS) to a dataset. How would you approach this task and what insights can be gained from the results?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  To apply Multidimensional Scaling (MDS) to a dataset, you would typically follow these steps:\n",
      "\n",
      "1. **Data Preparation**: Ensure that your dataset is in a suitable format for MDS analysis. MDS is used to visualize the level of similarity or dissimilarity between data points, so your dataset should consist of pairwise dissimilarity or similarity measures between the data points.\n",
      "\n",
      "2. **Choosing a Distance Metric**: Decide on the appropriate distance metric to use for calculating the dissimilarities between data points. Common distance metrics include Euclidean distance, Manhattan distance, or correlation-based distances.\n",
      "\n",
      "3. **Performing MDS Analysis**: Use software or programming tools that support MDS analysis to calculate the spatial representation of the data points based on their dissimilarities. MDS aims to represent the data in a lower-dimensional space while preserving the original pairwise distances as much as possible.\n",
      "\n",
      "4. **Interpreting the Results**: Once the MDS analysis is complete, you will obtain a visual representation of the data points in a lower-dimensional space. Insights gained from the results include:\n",
      "   - **Cluster Identification**: MDS can help identify clusters or groups of data points that are similar to each other.\n",
      "   - **Outlier Detection**: Outliers in the dataset may be visually apparent in the MDS plot.\n",
      "   - **Dimensionality Reduction**: MDS reduces the dimensionality of the data while preserving the relationships between data points, making it easier to interpret and visualize.\n",
      "\n",
      "5. **Validation and Interpretation**: Validate the MDS results by assessing how well the lower-dimensional representation captures the original dissimilarities in the data. Interpret the spatial arrangement of data points in the MDS plot to gain insights into the underlying structure of the dataset.\n",
      "\n",
      "By following these steps, you can effectively apply Multidimensional Scaling (MDS) to a dataset and gain valuable insights into the relationships between data points.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you are looking for information related to eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA) and how they are calculated. If you have a specific question or need clarification on any part of the information provided, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  10. Code Debugging: Identify and correct any errors in the following code snippet related to solving eigenvalue problems iteratively:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I cannot see the actual code snippet you are referring to in the provided context. If you could provide the code snippet, I would be happy to help you identify and correct any errors in it.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  Based on the provided context, it seems like you're looking for information on eigenvalues and eigenvectors in the context of Principal Component Analysis (PCA). Eigenvalues are the variance of principal components, and eigenvectors are the directions associated with these variances. The process of finding principal components involves calculating the eigenvectors and eigenvalues of the data covariance matrix. If you have any specific questions or need further clarification, feel free to ask!\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  ```python\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but it seems like you pasted Python code. How can I assist you with it?\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  def iterative_eigenvalue_solver(matrix):\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have the specific details on how to implement an iterative eigenvalue solver in Python.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:      eigenvalues = []\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't have the specific eigenvalues from the context provided.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:      for i in range(len(matrix)):\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The code snippet you provided seems to be a Python loop that iterates over the length of a matrix. However, based on the context provided, the matrix operations and calculations mentioned are more related to linear algebra and matrix manipulation rather than Python programming. If you have a specific question about implementing a matrix operation or calculation in Python, please provide more details so I can assist you further.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:          eigenvalues.append(np.linalg.eig(matrix[i]))\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The code you provided seems to be appending the eigenvalues of a matrix to a list named `eigenvalues`. The `np.linalg.eig()` function is likely being used to calculate the eigenvalues of the matrix at index `i`.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:      return eigenvalues\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  To calculate eigenvalues, you would typically need to follow the steps outlined in the context provided. This involves computing the eigenvalues of a square matrix by solving the characteristic equation det(A - Î»I) = 0, where A is the matrix, Î» is the eigenvalue, and I is the identity matrix. If you have a specific matrix for which you want to find the eigenvalues, you can apply this method.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't have enough information to provide a response.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  matrix = np.array([[1, 2], [3, 4]])\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  result = iterative_eigenvalue_solver(matrix)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  The function `iterative_eigenvalue_solver(matrix)` is likely designed to calculate the eigenvalues of a matrix using iterative techniques. The process involves finding the eigenvalues of a matrix through iterative methods rather than direct calculation. The specific details of the iterative method used in this function are not provided in the context given.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  print(result)\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I'm sorry, but I don't have access to the specific code or data you are referring to in order to provide you with the printed result. If you can provide more context or details, I may be able to help you further.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Question:  ```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer each question and save to a file\n",
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    result = answer_generation_chain(question)\n",
    "    answer = result['result']\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\n\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\n\")\n",
    "        f.write(\"--------------------------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
